3. Hướng dẫn cấu hình 2 máy LAN (chỉ tham khảo, không áp dụng)
Nếu bạn muốn 2 máy vật lý kết nối qua LAN và chạy Hadoop cluster:

Bước 1: Chuẩn bị mạng
Máy 1 (Master): 192.168.1.100
Máy 2 (Worker): 192.168.1.101

Bước 2: Cấu hình trên Máy 1 (Master)
Tạo file docker-compose-master.yml:
services:
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    ports:
      - "9870:9870"
      - "9000:9000"
    environment:
      - CLUSTER_NAME=retail-cluster
    env_file:
      - ./config/hadoop.env
    networks:
      bigdata-network:
        ipv4_address: 172.20.0.2

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    ports:
      - "9864:9864"
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    env_file:
      - ./config/hadoop.env
    networks:
      bigdata-network:
        ipv4_address: 172.20.0.3

  spark-master:
    image: bde2020/spark-master:3.3.0-hadoop3.3
    container_name: spark-master
    ports:
      - "8080:8080"
      - "7077:7077"
    networks:
      bigdata-network:
        ipv4_address: 172.20.0.10

  spark-worker:
    image: bde2020/spark-worker:3.3.0-hadoop3.3
    container_name: spark-worker
    environment:
      - SPARK_MASTER=spark://spark-master:7077
    networks:
      bigdata-network:
        ipv4_address: 172.20.0.11

networks:
  bigdata-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16

Bước 3: Cấu hình trên Máy 2 (Worker)
Tạo file docker-compose-worker.yml:
services:
  datanode2:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode2
    ports:
      - "9864:9864"
    environment:
      # Trỏ đến Namenode trên Máy 1
      SERVICE_PRECONDITION: "192.168.1.100:9870"
    env_file:
      - ./config/hadoop.env
    extra_hosts:
      - "namenode:192.168.1.100"
    networks:
      - bigdata-network

  spark-worker-2:
    image: bde2020/spark-worker:3.3.0-hadoop3.3
    container_name: spark-worker-2
    environment:
      # Trỏ đến Spark Master trên Máy 1
      - SPARK_MASTER=spark://192.168.1.100:7077
    extra_hosts:
      - "spark-master:192.168.1.100"
      - "namenode:192.168.1.100"
    networks:
      - bigdata-network

networks:
  bigdata-network:
    driver: bridge

Bước 4: Cập nhật file hadoop.env trên cả 2 máy
CORE_CONF_fs_defaultFS=hdfs://192.168.1.100:9000
HDFS_CONF_dfs_replication=2


Bước 5: Khởi động
# Trên Máy 1 (Master)
docker-compose -f docker-compose-master.yml up -d

# Trên Máy 2 (Worker)
docker-compose -f docker-compose-worker.yml up -d


Lưu ý quan trọng khi dùng 2 máy LAN:
Firewall: Mở các port cần thiết (9000, 9870, 9864, 7077, 8080)
DNS/Hosts: Đảm bảo 2 máy có thể phân giải tên của nhau
Network latency: LAN nên có tốc độ cao (Gigabit) để giảm độ trễ
Docker Swarm/Kubernetes: Nếu muốn quản lý tốt hơn, có thể dùng Docker Swarm hoặc K8s