services:
  # ==================== HADOOP CLUSTER ====================
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    restart: unless-stopped
    ports:
      - "9870:9870"
      - "9000:9000"
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
      - ./data:/data
    environment:
      - CLUSTER_NAME=retail-cluster
    env_file:
      - ./config/hadoop.env
    networks:
      - bigdata-network

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    restart: unless-stopped
    ports:
      - "9864:9864"
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    env_file:
      - ./config/hadoop.env
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
    networks:
      - bigdata-network
    depends_on:
      - namenode

  datanode2:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode2
    restart: unless-stopped
    ports:
      - "9865:9864"
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    env_file:
      - ./config/hadoop.env
    volumes:
      - hadoop_datanode2:/hadoop/dfs/data
    networks:
      - bigdata-network
    depends_on:
      - namenode

  # ==================== SPARK CLUSTER ====================
  spark-master:
    image: bde2020/spark-master:3.3.0-hadoop3.3
    container_name: spark-master
    restart: unless-stopped
    ports:
      - "8580:8080"
      - "7777:7077"
    environment:
      - INIT_DAEMON_STEP=setup_spark
    volumes:
      - ./spark-apps:/spark-apps
      - ./data:/data
    networks:
      - bigdata-network
    depends_on:
      - namenode
      - datanode
      - datanode2

  spark-worker:
    image: bde2020/spark-worker:3.3.0-hadoop3.3
    container_name: spark-worker
    restart: unless-stopped
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2G
    ports:
      - "8581:8081"
    volumes:
      - ./spark-apps:/spark-apps
      - ./data:/data
    networks:
      - bigdata-network
    depends_on:
      - spark-master

  spark-worker-2:
    image: bde2020/spark-worker:3.3.0-hadoop3.3
    container_name: spark-worker-2
    restart: unless-stopped
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2G
    ports:
      - "8582:8081"
    volumes:
      - ./spark-apps:/spark-apps
      - ./data:/data
    networks:
      - bigdata-network
    depends_on:
      - spark-master

  # ==================== MONGODB ====================
  mongodb:
    image: mongo:6.0
    container_name: mongodb
    restart: unless-stopped
    ports:
      - "27017:27017"
    environment:
      MONGO_INITDB_ROOT_USERNAME: admin
      MONGO_INITDB_ROOT_PASSWORD: admin123
      MONGO_INITDB_DATABASE: retail_analytics
    volumes:
      - mongodb_data:/data/db
      - ./mongo-init:/docker-entrypoint-initdb.d
    networks:
      - bigdata-network
    healthcheck:
      test: echo 'db.runCommand("ping").ok' | mongosh localhost:27017/admin --quiet
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 40s

  mongo-express:
    image: mongo-express:1.0.2-20-alpine3.19
    container_name: mongo-express
    restart: unless-stopped
    ports:
      - "8290:8081"
    environment:
      ME_CONFIG_MONGODB_ADMINUSERNAME: admin
      ME_CONFIG_MONGODB_ADMINPASSWORD: admin123
      ME_CONFIG_MONGODB_URL: mongodb://admin:admin123@mongodb:27017/
      ME_CONFIG_MONGODB_SERVER: mongodb
      ME_CONFIG_BASICAUTH_USERNAME: admin
      ME_CONFIG_BASICAUTH_PASSWORD: admin123
    networks:
      - bigdata-network
    depends_on:
      mongodb:
        condition: service_started

  # ==================== PYTHON WEB APPLICATION ====================
  webapp:
    build:
      context: ./webapp
      dockerfile: Dockerfile
    container_name: webapp
    restart: unless-stopped
    ports:
      - "5000:5000"
    environment:
      - MONGO_URI=mongodb://admin:admin123@mongodb:27017/?authSource=admin
      - FLASK_ENV=production
    networks:
      - bigdata-network
    depends_on:
      - mongodb
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/"]
      interval: 30s
      timeout: 10s
      retries: 3

networks:
  bigdata-network:
    driver: bridge

volumes:
  hadoop_namenode:
  hadoop_datanode:
  hadoop_datanode2:
  mongodb_data:
