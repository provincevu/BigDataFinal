{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e73a6165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Thiáº¿t láº­p style cho plots\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print('âœ… Libraries imported successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83f59380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Spark Session created!\n",
      "ðŸ“ Spark Version: 3.3.0\n",
      "ðŸ“ Application ID: app-20251221153928-0005\n"
     ]
    }
   ],
   "source": [
    "# Táº¡o Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RetailAnalysis\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"/user/hive/warehouse\") \\\n",
    "    .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(f'âœ… Spark Session created!')\n",
    "print(f'ðŸ“ Spark Version: {spark.version}')\n",
    "print(f'ðŸ“ Application ID: {spark.sparkContext.applicationId}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24de06d8",
   "metadata": {},
   "source": [
    "## 1. Load vÃ  KhÃ¡m phÃ¡ Dá»¯ liá»‡u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0acf806",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o50.load.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 11) (172.18.0.11 executor 0): java.io.FileNotFoundException: \nFile file:/home/jovyan/data/online_retail.csv does not exist\n\nIt is possible the underlying files have been updated. You can explicitly invalidate\nthe cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by\nrecreating the Dataset/DataFrame involved.\n       \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:648)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:212)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:270)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:112)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:65)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:62)\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:210)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:207)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:411)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.io.FileNotFoundException: \nFile file:/home/jovyan/data/online_retail.csv does not exist\n\nIt is possible the underlying files have been updated. You can explicitly invalidate\nthe cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by\nrecreating the Dataset/DataFrame involved.\n       \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:648)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:212)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:270)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load dá»¯ liá»‡u tá»« CSV\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Thá»­ Ä‘á»c vá»›i format rÃµ rÃ ng hÆ¡n\u001b[39;00m\n\u001b[1;32m      3\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minferSchema\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m----> 6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/jovyan/data/online_retail.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mðŸ“Š Total records: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf\u001b[38;5;241m.\u001b[39mcount()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mðŸ“‹ Columns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:177\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o50.load.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 11) (172.18.0.11 executor 0): java.io.FileNotFoundException: \nFile file:/home/jovyan/data/online_retail.csv does not exist\n\nIt is possible the underlying files have been updated. You can explicitly invalidate\nthe cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by\nrecreating the Dataset/DataFrame involved.\n       \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:648)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:212)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:270)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:112)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:65)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:62)\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:210)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:207)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:411)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.io.FileNotFoundException: \nFile file:/home/jovyan/data/online_retail.csv does not exist\n\nIt is possible the underlying files have been updated. You can explicitly invalidate\nthe cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by\nrecreating the Dataset/DataFrame involved.\n       \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:648)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:212)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:270)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "# Load dá»¯ liá»‡u tá»« CSV\n",
    "# Thá»­ Ä‘á»c vá»›i format rÃµ rÃ ng hÆ¡n\n",
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"/home/jovyan/data/online_retail.csv\")\n",
    "print(f'ðŸ“Š Total records: {df.count():,}')\n",
    "print(f'ðŸ“‹ Columns: {df.columns}')\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da84cec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xem máº«u dá»¯ liá»‡u\n",
    "df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed2a18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thá»‘ng kÃª mÃ´ táº£\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b45b47",
   "metadata": {},
   "source": [
    "## 2. LÃ m sáº¡ch vÃ  Xá»­ lÃ½ Dá»¯ liá»‡u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3736a744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LÃ m sáº¡ch dá»¯ liá»‡u\n",
    "df_cleaned = df \\\n",
    "    .filter(col(\"CustomerID\").isNotNull()) \\\n",
    "    .filter(col(\"Quantity\") > 0) \\\n",
    "    .filter(col(\"UnitPrice\") > 0) \\\n",
    "    .filter(~col(\"InvoiceNo\").startswith(\"C\")) \\\n",
    "    .withColumn(\"InvoiceDate\", to_timestamp(col(\"InvoiceDate\"))) \\\n",
    "    .withColumn(\"CustomerID\", col(\"CustomerID\").cast(\"integer\")) \\\n",
    "    .withColumn(\"TotalAmount\", round(col(\"Quantity\") * col(\"UnitPrice\"), 2)) \\\n",
    "    .withColumn(\"Year\", year(col(\"InvoiceDate\"))) \\\n",
    "    .withColumn(\"Month\", month(col(\"InvoiceDate\"))) \\\n",
    "    .withColumn(\"DayOfWeek\", dayofweek(col(\"InvoiceDate\"))) \\\n",
    "    .withColumn(\"Hour\", hour(col(\"InvoiceDate\")))\n",
    "\n",
    "print(f'ðŸ“Š Records after cleaning: {df_cleaned.count():,}')\n",
    "print(f'ðŸ“Š Unique customers: {df_cleaned.select(\"CustomerID\").distinct().count():,}')\n",
    "print(f'ðŸ“Š Unique products: {df_cleaned.select(\"StockCode\").distinct().count():,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af08051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache dá»¯ liá»‡u Ä‘á»ƒ tÄƒng tá»‘c xá»­ lÃ½\n",
    "df_cleaned.cache()\n",
    "df_cleaned.createOrReplaceTempView(\"transactions\")\n",
    "print('âœ… Data cached and temp view created!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64646f34",
   "metadata": {},
   "source": [
    "## 3. PhÃ¢n tÃ­ch Doanh thu theo Thá»i gian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f600558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doanh thu theo thÃ¡ng\n",
    "monthly_revenue = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        Year,\n",
    "        Month,\n",
    "        COUNT(DISTINCT InvoiceNo) as TotalOrders,\n",
    "        COUNT(DISTINCT CustomerID) as TotalCustomers,\n",
    "        ROUND(SUM(TotalAmount), 2) as TotalRevenue\n",
    "    FROM transactions\n",
    "    GROUP BY Year, Month\n",
    "    ORDER BY Year, Month\n",
    "\"\"\")\n",
    "\n",
    "monthly_revenue_pd = monthly_revenue.toPandas()\n",
    "monthly_revenue_pd['Period'] = monthly_revenue_pd['Year'].astype(str) + '-' + monthly_revenue_pd['Month'].astype(str).str.zfill(2)\n",
    "\n",
    "# Váº½ biá»ƒu Ä‘á»“\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Revenue trend\n",
    "axes[0].plot(monthly_revenue_pd['Period'], monthly_revenue_pd['TotalRevenue'], marker='o', linewidth=2, markersize=8)\n",
    "axes[0].set_title('ðŸ“ˆ Doanh thu theo ThÃ¡ng', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Thá»i gian')\n",
    "axes[0].set_ylabel('Doanh thu ($)')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Orders trend\n",
    "axes[1].bar(monthly_revenue_pd['Period'], monthly_revenue_pd['TotalOrders'], color='steelblue', alpha=0.7)\n",
    "axes[1].set_title('ðŸ“¦ Sá»‘ Ä‘Æ¡n hÃ ng theo ThÃ¡ng', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Thá»i gian')\n",
    "axes[1].set_ylabel('Sá»‘ Ä‘Æ¡n hÃ ng')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b879f6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doanh thu theo ngÃ y trong tuáº§n\n",
    "daily_revenue = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        DayOfWeek,\n",
    "        CASE DayOfWeek\n",
    "            WHEN 1 THEN 'Sunday'\n",
    "            WHEN 2 THEN 'Monday'\n",
    "            WHEN 3 THEN 'Tuesday'\n",
    "            WHEN 4 THEN 'Wednesday'\n",
    "            WHEN 5 THEN 'Thursday'\n",
    "            WHEN 6 THEN 'Friday'\n",
    "            WHEN 7 THEN 'Saturday'\n",
    "        END as DayName,\n",
    "        COUNT(DISTINCT InvoiceNo) as TotalOrders,\n",
    "        ROUND(SUM(TotalAmount), 2) as TotalRevenue\n",
    "    FROM transactions\n",
    "    GROUP BY DayOfWeek\n",
    "    ORDER BY DayOfWeek\n",
    "\"\"\")\n",
    "\n",
    "daily_revenue_pd = daily_revenue.toPandas()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars = ax.bar(daily_revenue_pd['DayName'], daily_revenue_pd['TotalRevenue'], color='coral', alpha=0.8)\n",
    "ax.set_title('ðŸ“… Doanh thu theo NgÃ y trong Tuáº§n', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('NgÃ y')\n",
    "ax.set_ylabel('Doanh thu ($)')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# ThÃªm labels\n",
    "for bar, val in zip(bars, daily_revenue_pd['TotalRevenue']):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10000, \n",
    "            f'${val:,.0f}', ha='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4292fa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doanh thu theo giá»\n",
    "hourly_revenue = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        Hour,\n",
    "        COUNT(DISTINCT InvoiceNo) as TotalOrders,\n",
    "        ROUND(SUM(TotalAmount), 2) as TotalRevenue\n",
    "    FROM transactions\n",
    "    GROUP BY Hour\n",
    "    ORDER BY Hour\n",
    "\"\"\")\n",
    "\n",
    "hourly_revenue_pd = hourly_revenue.toPandas()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.fill_between(hourly_revenue_pd['Hour'], hourly_revenue_pd['TotalRevenue'], alpha=0.3)\n",
    "ax.plot(hourly_revenue_pd['Hour'], hourly_revenue_pd['TotalRevenue'], marker='o', linewidth=2)\n",
    "ax.set_title('â° Doanh thu theo Giá» trong NgÃ y', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Giá»')\n",
    "ax.set_ylabel('Doanh thu ($)')\n",
    "ax.set_xticks(range(0, 24))\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25c4489",
   "metadata": {},
   "source": [
    "## 4. Top Sáº£n pháº©m BÃ¡n cháº¡y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4a0a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 15 sáº£n pháº©m theo doanh thu\n",
    "top_products = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        StockCode,\n",
    "        FIRST(Description) as Description,\n",
    "        SUM(Quantity) as TotalQuantity,\n",
    "        ROUND(SUM(TotalAmount), 2) as TotalRevenue,\n",
    "        COUNT(DISTINCT CustomerID) as UniqueCustomers\n",
    "    FROM transactions\n",
    "    GROUP BY StockCode\n",
    "    ORDER BY TotalRevenue DESC\n",
    "    LIMIT 15\n",
    "\"\"\")\n",
    "\n",
    "top_products_pd = top_products.toPandas()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "bars = ax.barh(range(len(top_products_pd)), top_products_pd['TotalRevenue'], color='teal', alpha=0.8)\n",
    "ax.set_yticks(range(len(top_products_pd)))\n",
    "ax.set_yticklabels([f\"{code}\\n{desc[:30]}...\" if len(str(desc)) > 30 else f\"{code}\\n{desc}\" \n",
    "                   for code, desc in zip(top_products_pd['StockCode'], top_products_pd['Description'])])\n",
    "ax.set_title('ðŸ† Top 15 Sáº£n pháº©m theo Doanh thu', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Doanh thu ($)')\n",
    "ax.invert_yaxis()\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "for bar, val in zip(bars, top_products_pd['TotalRevenue']):\n",
    "    ax.text(val + 1000, bar.get_y() + bar.get_height()/2, f'${val:,.0f}', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6fba5a",
   "metadata": {},
   "source": [
    "## 5. PhÃ¢n tÃ­ch KhÃ¡ch hÃ ng - RFM Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e408c471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TÃ­nh RFM cho tá»«ng khÃ¡ch hÃ ng\n",
    "rfm_df = spark.sql(\"\"\"\n",
    "    WITH max_date AS (\n",
    "        SELECT MAX(InvoiceDate) as MaxDate FROM transactions\n",
    "    )\n",
    "    SELECT \n",
    "        CustomerID,\n",
    "        DATEDIFF((SELECT MaxDate FROM max_date), MAX(InvoiceDate)) as Recency,\n",
    "        COUNT(DISTINCT InvoiceNo) as Frequency,\n",
    "        ROUND(SUM(TotalAmount), 2) as Monetary\n",
    "    FROM transactions\n",
    "    GROUP BY CustomerID\n",
    "\"\"\")\n",
    "\n",
    "rfm_pd = rfm_df.toPandas()\n",
    "\n",
    "# Váº½ phÃ¢n phá»‘i RFM\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axes[0].hist(rfm_pd['Recency'], bins=50, color='steelblue', alpha=0.7, edgecolor='white')\n",
    "axes[0].set_title('ðŸ“… PhÃ¢n phá»‘i Recency', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Sá»‘ ngÃ y tá»« láº§n mua cuá»‘i')\n",
    "axes[0].set_ylabel('Sá»‘ khÃ¡ch hÃ ng')\n",
    "\n",
    "axes[1].hist(rfm_pd['Frequency'], bins=50, color='coral', alpha=0.7, edgecolor='white')\n",
    "axes[1].set_title('ðŸ”„ PhÃ¢n phá»‘i Frequency', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Sá»‘ láº§n mua hÃ ng')\n",
    "axes[1].set_ylabel('Sá»‘ khÃ¡ch hÃ ng')\n",
    "\n",
    "axes[2].hist(rfm_pd['Monetary'], bins=50, color='teal', alpha=0.7, edgecolor='white')\n",
    "axes[2].set_title('ðŸ’° PhÃ¢n phá»‘i Monetary', fontsize=12, fontweight='bold')\n",
    "axes[2].set_xlabel('Tá»•ng chi tiÃªu ($)')\n",
    "axes[2].set_ylabel('Sá»‘ khÃ¡ch hÃ ng')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97066cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PhÃ¢n khÃºc khÃ¡ch hÃ ng theo RFM Score\n",
    "customer_segments = spark.sql(\"\"\"\n",
    "    WITH rfm_base AS (\n",
    "        SELECT \n",
    "            CustomerID,\n",
    "            DATEDIFF(\n",
    "                (SELECT MAX(InvoiceDate) FROM transactions),\n",
    "                MAX(InvoiceDate)\n",
    "            ) as Recency,\n",
    "            COUNT(DISTINCT InvoiceNo) as Frequency,\n",
    "            SUM(TotalAmount) as Monetary\n",
    "        FROM transactions\n",
    "        GROUP BY CustomerID\n",
    "    ),\n",
    "    rfm_scores AS (\n",
    "        SELECT \n",
    "            *,\n",
    "            NTILE(5) OVER (ORDER BY Recency DESC) as R_Score,\n",
    "            NTILE(5) OVER (ORDER BY Frequency) as F_Score,\n",
    "            NTILE(5) OVER (ORDER BY Monetary) as M_Score\n",
    "        FROM rfm_base\n",
    "    )\n",
    "    SELECT \n",
    "        CASE \n",
    "            WHEN R_Score >= 4 AND F_Score >= 4 AND M_Score >= 4 THEN 'Champions'\n",
    "            WHEN R_Score >= 3 AND F_Score >= 3 AND M_Score >= 3 THEN 'Loyal Customers'\n",
    "            WHEN R_Score >= 4 AND F_Score <= 2 THEN 'New Customers'\n",
    "            WHEN R_Score <= 2 AND F_Score >= 3 THEN 'At Risk'\n",
    "            WHEN R_Score <= 2 AND F_Score <= 2 AND M_Score <= 2 THEN 'Lost'\n",
    "            ELSE 'Regular'\n",
    "        END as Segment,\n",
    "        COUNT(*) as CustomerCount,\n",
    "        ROUND(AVG(Monetary), 2) as AvgMonetary,\n",
    "        ROUND(SUM(Monetary), 2) as TotalMonetary\n",
    "    FROM rfm_scores\n",
    "    GROUP BY \n",
    "        CASE \n",
    "            WHEN R_Score >= 4 AND F_Score >= 4 AND M_Score >= 4 THEN 'Champions'\n",
    "            WHEN R_Score >= 3 AND F_Score >= 3 AND M_Score >= 3 THEN 'Loyal Customers'\n",
    "            WHEN R_Score >= 4 AND F_Score <= 2 THEN 'New Customers'\n",
    "            WHEN R_Score <= 2 AND F_Score >= 3 THEN 'At Risk'\n",
    "            WHEN R_Score <= 2 AND F_Score <= 2 AND M_Score <= 2 THEN 'Lost'\n",
    "            ELSE 'Regular'\n",
    "        END\n",
    "    ORDER BY TotalMonetary DESC\n",
    "\"\"\")\n",
    "\n",
    "segments_pd = customer_segments.toPandas()\n",
    "print('ðŸ“Š Customer Segments:')\n",
    "print(segments_pd)\n",
    "\n",
    "# Váº½ pie chart\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "colors = ['#2ecc71', '#3498db', '#9b59b6', '#e74c3c', '#95a5a6', '#f39c12']\n",
    "axes[0].pie(segments_pd['CustomerCount'], labels=segments_pd['Segment'], autopct='%1.1f%%', \n",
    "            colors=colors, startangle=90)\n",
    "axes[0].set_title('ðŸ‘¥ PhÃ¢n bá»‘ KhÃ¡ch hÃ ng theo Segment', fontsize=12, fontweight='bold')\n",
    "\n",
    "axes[1].pie(segments_pd['TotalMonetary'], labels=segments_pd['Segment'], autopct='%1.1f%%', \n",
    "            colors=colors, startangle=90)\n",
    "axes[1].set_title('ðŸ’° Doanh thu theo Segment', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dd8dc1",
   "metadata": {},
   "source": [
    "## 6. Customer Clustering vá»›i K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c2a9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chuáº©n bá»‹ features cho clustering\n",
    "customer_features = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        CustomerID,\n",
    "        DATEDIFF(\n",
    "            (SELECT MAX(InvoiceDate) FROM transactions),\n",
    "            MAX(InvoiceDate)\n",
    "        ) as Recency,\n",
    "        COUNT(DISTINCT InvoiceNo) as Frequency,\n",
    "        ROUND(SUM(TotalAmount), 2) as Monetary,\n",
    "        COUNT(DISTINCT StockCode) as UniqueProducts,\n",
    "        ROUND(AVG(Quantity), 2) as AvgQuantity\n",
    "    FROM transactions\n",
    "    GROUP BY CustomerID\n",
    "    HAVING COUNT(DISTINCT InvoiceNo) >= 2\n",
    "\"\"\")\n",
    "\n",
    "print(f'âœ… Customers for clustering: {customer_features.count():,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d64d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Assembler\n",
    "feature_cols = ['Recency', 'Frequency', 'Monetary', 'UniqueProducts', 'AvgQuantity']\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "df_vector = assembler.transform(customer_features)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "scaler_model = scaler.fit(df_vector)\n",
    "df_scaled = scaler_model.transform(df_vector)\n",
    "\n",
    "print('âœ… Features assembled and scaled!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62d208a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TÃ¬m sá»‘ cluster tá»‘i Æ°u (Elbow method)\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "costs = []\n",
    "silhouettes = []\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "for k in range(2, 11):\n",
    "    kmeans = KMeans().setK(k).setSeed(42).setFeaturesCol(\"scaled_features\")\n",
    "    model = kmeans.fit(df_scaled)\n",
    "    \n",
    "    cost = model.summary.trainingCost\n",
    "    predictions = model.transform(df_scaled)\n",
    "    silhouette = evaluator.evaluate(predictions)\n",
    "    \n",
    "    costs.append(cost)\n",
    "    silhouettes.append(silhouette)\n",
    "    print(f'K={k}: Cost={cost:.2f}, Silhouette={silhouette:.4f}')\n",
    "\n",
    "# Plot elbow\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axes[0].plot(range(2, 11), costs, marker='o', linewidth=2)\n",
    "axes[0].set_title('ðŸ“‰ Elbow Method', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Number of Clusters (K)')\n",
    "axes[0].set_ylabel('Cost (WSSSE)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(range(2, 11), silhouettes, marker='o', linewidth=2, color='coral')\n",
    "axes[1].set_title('ðŸ“Š Silhouette Score', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Number of Clusters (K)')\n",
    "axes[1].set_ylabel('Silhouette Score')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3f51c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model vá»›i K=5\n",
    "final_k = 5\n",
    "kmeans = KMeans().setK(final_k).setSeed(42).setFeaturesCol(\"scaled_features\").setPredictionCol(\"Cluster\")\n",
    "model = kmeans.fit(df_scaled)\n",
    "clustered = model.transform(df_scaled)\n",
    "\n",
    "# Cluster statistics\n",
    "cluster_stats = clustered.groupBy('Cluster').agg(\n",
    "    count('*').alias('CustomerCount'),\n",
    "    round(avg('Recency'), 1).alias('AvgRecency'),\n",
    "    round(avg('Frequency'), 1).alias('AvgFrequency'),\n",
    "    round(avg('Monetary'), 2).alias('AvgMonetary'),\n",
    "    round(avg('UniqueProducts'), 1).alias('AvgProducts')\n",
    ").orderBy('Cluster')\n",
    "\n",
    "cluster_stats.show()\n",
    "\n",
    "# Visualize cluster distribution\n",
    "cluster_pd = cluster_stats.toPandas()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars = ax.bar(cluster_pd['Cluster'].astype(str), cluster_pd['CustomerCount'], color='steelblue', alpha=0.8)\n",
    "ax.set_title('ðŸ‘¥ Sá»‘ lÆ°á»£ng KhÃ¡ch hÃ ng theo Cluster', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Cluster')\n",
    "ax.set_ylabel('Sá»‘ khÃ¡ch hÃ ng')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, val in zip(bars, cluster_pd['CustomerCount']):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10, \n",
    "            f'{val:,}', ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337b816d",
   "metadata": {},
   "source": [
    "## 7. PhÃ¢n tÃ­ch theo Quá»‘c gia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b24306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 quá»‘c gia theo doanh thu\n",
    "country_stats = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        Country,\n",
    "        COUNT(DISTINCT CustomerID) as TotalCustomers,\n",
    "        COUNT(DISTINCT InvoiceNo) as TotalOrders,\n",
    "        ROUND(SUM(TotalAmount), 2) as TotalRevenue\n",
    "    FROM transactions\n",
    "    GROUP BY Country\n",
    "    ORDER BY TotalRevenue DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "country_pd = country_stats.toPandas()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars = ax.barh(range(len(country_pd)), country_pd['TotalRevenue'], color='teal', alpha=0.8)\n",
    "ax.set_yticks(range(len(country_pd)))\n",
    "ax.set_yticklabels(country_pd['Country'])\n",
    "ax.set_title('ðŸŒ Top 10 Quá»‘c gia theo Doanh thu', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Doanh thu ($)')\n",
    "ax.invert_yaxis()\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "for bar, val in zip(bars, country_pd['TotalRevenue']):\n",
    "    ax.text(val + 10000, bar.get_y() + bar.get_height()/2, f'${val:,.0f}', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ff1192",
   "metadata": {},
   "source": [
    "## 8. Tá»•ng káº¿t vÃ  LÆ°u káº¿t quáº£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68a14e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tá»•ng quan\n",
    "print('=' * 60)\n",
    "print('ðŸ“‹ Tá»”NG Káº¾T PHÃ‚N TÃCH')\n",
    "print('=' * 60)\n",
    "\n",
    "total_records = df_cleaned.count()\n",
    "total_customers = df_cleaned.select('CustomerID').distinct().count()\n",
    "total_products = df_cleaned.select('StockCode').distinct().count()\n",
    "total_revenue = df_cleaned.agg({'TotalAmount': 'sum'}).collect()[0][0]\n",
    "total_orders = df_cleaned.select('InvoiceNo').distinct().count()\n",
    "\n",
    "print(f'ðŸ“Š Tá»•ng sá»‘ giao dá»‹ch: {total_records:,}')\n",
    "print(f'ðŸ‘¥ Tá»•ng sá»‘ khÃ¡ch hÃ ng: {total_customers:,}')\n",
    "print(f'ðŸ“¦ Tá»•ng sá»‘ sáº£n pháº©m: {total_products:,}')\n",
    "print(f'ðŸ§¾ Tá»•ng sá»‘ Ä‘Æ¡n hÃ ng: {total_orders:,}')\n",
    "print(f'ðŸ’° Tá»•ng doanh thu: ${total_revenue:,.2f}')\n",
    "print(f'ðŸ’µ GiÃ¡ trá»‹ Ä‘Æ¡n hÃ ng TB: ${total_revenue/total_orders:,.2f}')\n",
    "print('=' * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0411a118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dá»«ng Spark Session\n",
    "spark.stop()\n",
    "print('âœ… Spark Session stopped!')\n",
    "print('ðŸŽ‰ Analysis completed successfully!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
