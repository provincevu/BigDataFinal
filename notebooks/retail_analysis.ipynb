{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73a6165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Thi·∫øt l·∫≠p style cho plots\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print('‚úÖ Libraries imported successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f59380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·∫°o Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RetailAnalysis\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"/user/hive/warehouse\") \\\n",
    "    .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(f'‚úÖ Spark Session created!')\n",
    "print(f'üìç Spark Version: {spark.version}')\n",
    "print(f'üìç Application ID: {spark.sparkContext.applicationId}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24de06d8",
   "metadata": {},
   "source": [
    "## 1. Load v√† Kh√°m ph√° D·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0acf806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load d·ªØ li·ªáu t·ª´ CSV\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"/home/jovyan/data/online_retail.csv\")\n",
    "\n",
    "print(f'üìä Total records: {df.count():,}')\n",
    "print(f'üìã Columns: {df.columns}')\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da84cec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xem m·∫´u d·ªØ li·ªáu\n",
    "df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed2a18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Th·ªëng k√™ m√¥ t·∫£\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b45b47",
   "metadata": {},
   "source": [
    "## 2. L√†m s·∫°ch v√† X·ª≠ l√Ω D·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3736a744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L√†m s·∫°ch d·ªØ li·ªáu\n",
    "df_cleaned = df \\\n",
    "    .filter(col(\"CustomerID\").isNotNull()) \\\n",
    "    .filter(col(\"Quantity\") > 0) \\\n",
    "    .filter(col(\"UnitPrice\") > 0) \\\n",
    "    .filter(~col(\"InvoiceNo\").startswith(\"C\")) \\\n",
    "    .withColumn(\"InvoiceDate\", to_timestamp(col(\"InvoiceDate\"))) \\\n",
    "    .withColumn(\"CustomerID\", col(\"CustomerID\").cast(\"integer\")) \\\n",
    "    .withColumn(\"TotalAmount\", round(col(\"Quantity\") * col(\"UnitPrice\"), 2)) \\\n",
    "    .withColumn(\"Year\", year(col(\"InvoiceDate\"))) \\\n",
    "    .withColumn(\"Month\", month(col(\"InvoiceDate\"))) \\\n",
    "    .withColumn(\"DayOfWeek\", dayofweek(col(\"InvoiceDate\"))) \\\n",
    "    .withColumn(\"Hour\", hour(col(\"InvoiceDate\")))\n",
    "\n",
    "print(f'üìä Records after cleaning: {df_cleaned.count():,}')\n",
    "print(f'üìä Unique customers: {df_cleaned.select(\"CustomerID\").distinct().count():,}')\n",
    "print(f'üìä Unique products: {df_cleaned.select(\"StockCode\").distinct().count():,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af08051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache d·ªØ li·ªáu ƒë·ªÉ tƒÉng t·ªëc x·ª≠ l√Ω\n",
    "df_cleaned.cache()\n",
    "df_cleaned.createOrReplaceTempView(\"transactions\")\n",
    "print('‚úÖ Data cached and temp view created!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64646f34",
   "metadata": {},
   "source": [
    "## 3. Ph√¢n t√≠ch Doanh thu theo Th·ªùi gian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f600558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doanh thu theo th√°ng\n",
    "monthly_revenue = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        Year,\n",
    "        Month,\n",
    "        COUNT(DISTINCT InvoiceNo) as TotalOrders,\n",
    "        COUNT(DISTINCT CustomerID) as TotalCustomers,\n",
    "        ROUND(SUM(TotalAmount), 2) as TotalRevenue\n",
    "    FROM transactions\n",
    "    GROUP BY Year, Month\n",
    "    ORDER BY Year, Month\n",
    "\"\"\")\n",
    "\n",
    "monthly_revenue_pd = monthly_revenue.toPandas()\n",
    "monthly_revenue_pd['Period'] = monthly_revenue_pd['Year'].astype(str) + '-' + monthly_revenue_pd['Month'].astype(str).str.zfill(2)\n",
    "\n",
    "# V·∫Ω bi·ªÉu ƒë·ªì\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Revenue trend\n",
    "axes[0].plot(monthly_revenue_pd['Period'], monthly_revenue_pd['TotalRevenue'], marker='o', linewidth=2, markersize=8)\n",
    "axes[0].set_title('üìà Doanh thu theo Th√°ng', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Th·ªùi gian')\n",
    "axes[0].set_ylabel('Doanh thu ($)')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Orders trend\n",
    "axes[1].bar(monthly_revenue_pd['Period'], monthly_revenue_pd['TotalOrders'], color='steelblue', alpha=0.7)\n",
    "axes[1].set_title('üì¶ S·ªë ƒë∆°n h√†ng theo Th√°ng', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Th·ªùi gian')\n",
    "axes[1].set_ylabel('S·ªë ƒë∆°n h√†ng')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b879f6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doanh thu theo ng√†y trong tu·∫ßn\n",
    "daily_revenue = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        DayOfWeek,\n",
    "        CASE DayOfWeek\n",
    "            WHEN 1 THEN 'Sunday'\n",
    "            WHEN 2 THEN 'Monday'\n",
    "            WHEN 3 THEN 'Tuesday'\n",
    "            WHEN 4 THEN 'Wednesday'\n",
    "            WHEN 5 THEN 'Thursday'\n",
    "            WHEN 6 THEN 'Friday'\n",
    "            WHEN 7 THEN 'Saturday'\n",
    "        END as DayName,\n",
    "        COUNT(DISTINCT InvoiceNo) as TotalOrders,\n",
    "        ROUND(SUM(TotalAmount), 2) as TotalRevenue\n",
    "    FROM transactions\n",
    "    GROUP BY DayOfWeek\n",
    "    ORDER BY DayOfWeek\n",
    "\"\"\")\n",
    "\n",
    "daily_revenue_pd = daily_revenue.toPandas()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars = ax.bar(daily_revenue_pd['DayName'], daily_revenue_pd['TotalRevenue'], color='coral', alpha=0.8)\n",
    "ax.set_title('üìÖ Doanh thu theo Ng√†y trong Tu·∫ßn', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Ng√†y')\n",
    "ax.set_ylabel('Doanh thu ($)')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Th√™m labels\n",
    "for bar, val in zip(bars, daily_revenue_pd['TotalRevenue']):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10000, \n",
    "            f'${val:,.0f}', ha='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4292fa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doanh thu theo gi·ªù\n",
    "hourly_revenue = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        Hour,\n",
    "        COUNT(DISTINCT InvoiceNo) as TotalOrders,\n",
    "        ROUND(SUM(TotalAmount), 2) as TotalRevenue\n",
    "    FROM transactions\n",
    "    GROUP BY Hour\n",
    "    ORDER BY Hour\n",
    "\"\"\")\n",
    "\n",
    "hourly_revenue_pd = hourly_revenue.toPandas()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.fill_between(hourly_revenue_pd['Hour'], hourly_revenue_pd['TotalRevenue'], alpha=0.3)\n",
    "ax.plot(hourly_revenue_pd['Hour'], hourly_revenue_pd['TotalRevenue'], marker='o', linewidth=2)\n",
    "ax.set_title('‚è∞ Doanh thu theo Gi·ªù trong Ng√†y', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Gi·ªù')\n",
    "ax.set_ylabel('Doanh thu ($)')\n",
    "ax.set_xticks(range(0, 24))\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25c4489",
   "metadata": {},
   "source": [
    "## 4. Top S·∫£n ph·∫©m B√°n ch·∫°y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4a0a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 15 s·∫£n ph·∫©m theo doanh thu\n",
    "top_products = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        StockCode,\n",
    "        FIRST(Description) as Description,\n",
    "        SUM(Quantity) as TotalQuantity,\n",
    "        ROUND(SUM(TotalAmount), 2) as TotalRevenue,\n",
    "        COUNT(DISTINCT CustomerID) as UniqueCustomers\n",
    "    FROM transactions\n",
    "    GROUP BY StockCode\n",
    "    ORDER BY TotalRevenue DESC\n",
    "    LIMIT 15\n",
    "\"\"\")\n",
    "\n",
    "top_products_pd = top_products.toPandas()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "bars = ax.barh(range(len(top_products_pd)), top_products_pd['TotalRevenue'], color='teal', alpha=0.8)\n",
    "ax.set_yticks(range(len(top_products_pd)))\n",
    "ax.set_yticklabels([f\"{code}\\n{desc[:30]}...\" if len(str(desc)) > 30 else f\"{code}\\n{desc}\" \n",
    "                   for code, desc in zip(top_products_pd['StockCode'], top_products_pd['Description'])])\n",
    "ax.set_title('üèÜ Top 15 S·∫£n ph·∫©m theo Doanh thu', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Doanh thu ($)')\n",
    "ax.invert_yaxis()\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "for bar, val in zip(bars, top_products_pd['TotalRevenue']):\n",
    "    ax.text(val + 1000, bar.get_y() + bar.get_height()/2, f'${val:,.0f}', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6fba5a",
   "metadata": {},
   "source": [
    "## 5. Ph√¢n t√≠ch Kh√°ch h√†ng - RFM Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e408c471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T√≠nh RFM cho t·ª´ng kh√°ch h√†ng\n",
    "rfm_df = spark.sql(\"\"\"\n",
    "    WITH max_date AS (\n",
    "        SELECT MAX(InvoiceDate) as MaxDate FROM transactions\n",
    "    )\n",
    "    SELECT \n",
    "        CustomerID,\n",
    "        DATEDIFF((SELECT MaxDate FROM max_date), MAX(InvoiceDate)) as Recency,\n",
    "        COUNT(DISTINCT InvoiceNo) as Frequency,\n",
    "        ROUND(SUM(TotalAmount), 2) as Monetary\n",
    "    FROM transactions\n",
    "    GROUP BY CustomerID\n",
    "\"\"\")\n",
    "\n",
    "rfm_pd = rfm_df.toPandas()\n",
    "\n",
    "# V·∫Ω ph√¢n ph·ªëi RFM\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axes[0].hist(rfm_pd['Recency'], bins=50, color='steelblue', alpha=0.7, edgecolor='white')\n",
    "axes[0].set_title('üìÖ Ph√¢n ph·ªëi Recency', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('S·ªë ng√†y t·ª´ l·∫ßn mua cu·ªëi')\n",
    "axes[0].set_ylabel('S·ªë kh√°ch h√†ng')\n",
    "\n",
    "axes[1].hist(rfm_pd['Frequency'], bins=50, color='coral', alpha=0.7, edgecolor='white')\n",
    "axes[1].set_title('üîÑ Ph√¢n ph·ªëi Frequency', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('S·ªë l·∫ßn mua h√†ng')\n",
    "axes[1].set_ylabel('S·ªë kh√°ch h√†ng')\n",
    "\n",
    "axes[2].hist(rfm_pd['Monetary'], bins=50, color='teal', alpha=0.7, edgecolor='white')\n",
    "axes[2].set_title('üí∞ Ph√¢n ph·ªëi Monetary', fontsize=12, fontweight='bold')\n",
    "axes[2].set_xlabel('T·ªïng chi ti√™u ($)')\n",
    "axes[2].set_ylabel('S·ªë kh√°ch h√†ng')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97066cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ph√¢n kh√∫c kh√°ch h√†ng theo RFM Score\n",
    "customer_segments = spark.sql(\"\"\"\n",
    "    WITH rfm_base AS (\n",
    "        SELECT \n",
    "            CustomerID,\n",
    "            DATEDIFF(\n",
    "                (SELECT MAX(InvoiceDate) FROM transactions),\n",
    "                MAX(InvoiceDate)\n",
    "            ) as Recency,\n",
    "            COUNT(DISTINCT InvoiceNo) as Frequency,\n",
    "            SUM(TotalAmount) as Monetary\n",
    "        FROM transactions\n",
    "        GROUP BY CustomerID\n",
    "    ),\n",
    "    rfm_scores AS (\n",
    "        SELECT \n",
    "            *,\n",
    "            NTILE(5) OVER (ORDER BY Recency DESC) as R_Score,\n",
    "            NTILE(5) OVER (ORDER BY Frequency) as F_Score,\n",
    "            NTILE(5) OVER (ORDER BY Monetary) as M_Score\n",
    "        FROM rfm_base\n",
    "    )\n",
    "    SELECT \n",
    "        CASE \n",
    "            WHEN R_Score >= 4 AND F_Score >= 4 AND M_Score >= 4 THEN 'Champions'\n",
    "            WHEN R_Score >= 3 AND F_Score >= 3 AND M_Score >= 3 THEN 'Loyal Customers'\n",
    "            WHEN R_Score >= 4 AND F_Score <= 2 THEN 'New Customers'\n",
    "            WHEN R_Score <= 2 AND F_Score >= 3 THEN 'At Risk'\n",
    "            WHEN R_Score <= 2 AND F_Score <= 2 AND M_Score <= 2 THEN 'Lost'\n",
    "            ELSE 'Regular'\n",
    "        END as Segment,\n",
    "        COUNT(*) as CustomerCount,\n",
    "        ROUND(AVG(Monetary), 2) as AvgMonetary,\n",
    "        ROUND(SUM(Monetary), 2) as TotalMonetary\n",
    "    FROM rfm_scores\n",
    "    GROUP BY \n",
    "        CASE \n",
    "            WHEN R_Score >= 4 AND F_Score >= 4 AND M_Score >= 4 THEN 'Champions'\n",
    "            WHEN R_Score >= 3 AND F_Score >= 3 AND M_Score >= 3 THEN 'Loyal Customers'\n",
    "            WHEN R_Score >= 4 AND F_Score <= 2 THEN 'New Customers'\n",
    "            WHEN R_Score <= 2 AND F_Score >= 3 THEN 'At Risk'\n",
    "            WHEN R_Score <= 2 AND F_Score <= 2 AND M_Score <= 2 THEN 'Lost'\n",
    "            ELSE 'Regular'\n",
    "        END\n",
    "    ORDER BY TotalMonetary DESC\n",
    "\"\"\")\n",
    "\n",
    "segments_pd = customer_segments.toPandas()\n",
    "print('üìä Customer Segments:')\n",
    "print(segments_pd)\n",
    "\n",
    "# V·∫Ω pie chart\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "colors = ['#2ecc71', '#3498db', '#9b59b6', '#e74c3c', '#95a5a6', '#f39c12']\n",
    "axes[0].pie(segments_pd['CustomerCount'], labels=segments_pd['Segment'], autopct='%1.1f%%', \n",
    "            colors=colors, startangle=90)\n",
    "axes[0].set_title('üë• Ph√¢n b·ªë Kh√°ch h√†ng theo Segment', fontsize=12, fontweight='bold')\n",
    "\n",
    "axes[1].pie(segments_pd['TotalMonetary'], labels=segments_pd['Segment'], autopct='%1.1f%%', \n",
    "            colors=colors, startangle=90)\n",
    "axes[1].set_title('üí∞ Doanh thu theo Segment', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dd8dc1",
   "metadata": {},
   "source": [
    "## 6. Customer Clustering v·ªõi K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c2a9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chu·∫©n b·ªã features cho clustering\n",
    "customer_features = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        CustomerID,\n",
    "        DATEDIFF(\n",
    "            (SELECT MAX(InvoiceDate) FROM transactions),\n",
    "            MAX(InvoiceDate)\n",
    "        ) as Recency,\n",
    "        COUNT(DISTINCT InvoiceNo) as Frequency,\n",
    "        ROUND(SUM(TotalAmount), 2) as Monetary,\n",
    "        COUNT(DISTINCT StockCode) as UniqueProducts,\n",
    "        ROUND(AVG(Quantity), 2) as AvgQuantity\n",
    "    FROM transactions\n",
    "    GROUP BY CustomerID\n",
    "    HAVING COUNT(DISTINCT InvoiceNo) >= 2\n",
    "\"\"\")\n",
    "\n",
    "print(f'‚úÖ Customers for clustering: {customer_features.count():,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d64d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Assembler\n",
    "feature_cols = ['Recency', 'Frequency', 'Monetary', 'UniqueProducts', 'AvgQuantity']\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "df_vector = assembler.transform(customer_features)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "scaler_model = scaler.fit(df_vector)\n",
    "df_scaled = scaler_model.transform(df_vector)\n",
    "\n",
    "print('‚úÖ Features assembled and scaled!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62d208a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T√¨m s·ªë cluster t·ªëi ∆∞u (Elbow method)\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "costs = []\n",
    "silhouettes = []\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "for k in range(2, 11):\n",
    "    kmeans = KMeans().setK(k).setSeed(42).setFeaturesCol(\"scaled_features\")\n",
    "    model = kmeans.fit(df_scaled)\n",
    "    \n",
    "    cost = model.summary.trainingCost\n",
    "    predictions = model.transform(df_scaled)\n",
    "    silhouette = evaluator.evaluate(predictions)\n",
    "    \n",
    "    costs.append(cost)\n",
    "    silhouettes.append(silhouette)\n",
    "    print(f'K={k}: Cost={cost:.2f}, Silhouette={silhouette:.4f}')\n",
    "\n",
    "# Plot elbow\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axes[0].plot(range(2, 11), costs, marker='o', linewidth=2)\n",
    "axes[0].set_title('üìâ Elbow Method', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Number of Clusters (K)')\n",
    "axes[0].set_ylabel('Cost (WSSSE)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(range(2, 11), silhouettes, marker='o', linewidth=2, color='coral')\n",
    "axes[1].set_title('üìä Silhouette Score', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Number of Clusters (K)')\n",
    "axes[1].set_ylabel('Silhouette Score')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3f51c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model v·ªõi K=5\n",
    "final_k = 5\n",
    "kmeans = KMeans().setK(final_k).setSeed(42).setFeaturesCol(\"scaled_features\").setPredictionCol(\"Cluster\")\n",
    "model = kmeans.fit(df_scaled)\n",
    "clustered = model.transform(df_scaled)\n",
    "\n",
    "# Cluster statistics\n",
    "cluster_stats = clustered.groupBy('Cluster').agg(\n",
    "    count('*').alias('CustomerCount'),\n",
    "    round(avg('Recency'), 1).alias('AvgRecency'),\n",
    "    round(avg('Frequency'), 1).alias('AvgFrequency'),\n",
    "    round(avg('Monetary'), 2).alias('AvgMonetary'),\n",
    "    round(avg('UniqueProducts'), 1).alias('AvgProducts')\n",
    ").orderBy('Cluster')\n",
    "\n",
    "cluster_stats.show()\n",
    "\n",
    "# Visualize cluster distribution\n",
    "cluster_pd = cluster_stats.toPandas()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars = ax.bar(cluster_pd['Cluster'].astype(str), cluster_pd['CustomerCount'], color='steelblue', alpha=0.8)\n",
    "ax.set_title('üë• S·ªë l∆∞·ª£ng Kh√°ch h√†ng theo Cluster', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Cluster')\n",
    "ax.set_ylabel('S·ªë kh√°ch h√†ng')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, val in zip(bars, cluster_pd['CustomerCount']):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10, \n",
    "            f'{val:,}', ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337b816d",
   "metadata": {},
   "source": [
    "## 7. Ph√¢n t√≠ch theo Qu·ªëc gia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b24306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 qu·ªëc gia theo doanh thu\n",
    "country_stats = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        Country,\n",
    "        COUNT(DISTINCT CustomerID) as TotalCustomers,\n",
    "        COUNT(DISTINCT InvoiceNo) as TotalOrders,\n",
    "        ROUND(SUM(TotalAmount), 2) as TotalRevenue\n",
    "    FROM transactions\n",
    "    GROUP BY Country\n",
    "    ORDER BY TotalRevenue DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "country_pd = country_stats.toPandas()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars = ax.barh(range(len(country_pd)), country_pd['TotalRevenue'], color='teal', alpha=0.8)\n",
    "ax.set_yticks(range(len(country_pd)))\n",
    "ax.set_yticklabels(country_pd['Country'])\n",
    "ax.set_title('üåç Top 10 Qu·ªëc gia theo Doanh thu', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Doanh thu ($)')\n",
    "ax.invert_yaxis()\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "for bar, val in zip(bars, country_pd['TotalRevenue']):\n",
    "    ax.text(val + 10000, bar.get_y() + bar.get_height()/2, f'${val:,.0f}', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ff1192",
   "metadata": {},
   "source": [
    "## 8. T·ªïng k·∫øt v√† L∆∞u k·∫øt qu·∫£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68a14e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·ªïng quan\n",
    "print('=' * 60)\n",
    "print('üìã T·ªîNG K·∫æT PH√ÇN T√çCH')\n",
    "print('=' * 60)\n",
    "\n",
    "total_records = df_cleaned.count()\n",
    "total_customers = df_cleaned.select('CustomerID').distinct().count()\n",
    "total_products = df_cleaned.select('StockCode').distinct().count()\n",
    "total_revenue = df_cleaned.agg({'TotalAmount': 'sum'}).collect()[0][0]\n",
    "total_orders = df_cleaned.select('InvoiceNo').distinct().count()\n",
    "\n",
    "print(f'üìä T·ªïng s·ªë giao d·ªãch: {total_records:,}')\n",
    "print(f'üë• T·ªïng s·ªë kh√°ch h√†ng: {total_customers:,}')\n",
    "print(f'üì¶ T·ªïng s·ªë s·∫£n ph·∫©m: {total_products:,}')\n",
    "print(f'üßæ T·ªïng s·ªë ƒë∆°n h√†ng: {total_orders:,}')\n",
    "print(f'üí∞ T·ªïng doanh thu: ${total_revenue:,.2f}')\n",
    "print(f'üíµ Gi√° tr·ªã ƒë∆°n h√†ng TB: ${total_revenue/total_orders:,.2f}')\n",
    "print('=' * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0411a118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D·ª´ng Spark Session\n",
    "spark.stop()\n",
    "print('‚úÖ Spark Session stopped!')\n",
    "print('üéâ Analysis completed successfully!')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
