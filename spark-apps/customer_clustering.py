"""
Customer Clustering Analysis using K-Means
============================================
Nh√≥m kh√°ch h√†ng c√≥ h√†nh vi mua s·∫Øm gi·ªëng nhau
s·ª≠ d·ª•ng thu·∫≠t to√°n K-Means Clustering
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, sum as spark_sum, count, avg, 
    max as spark_max, min as spark_min,
    datediff, round as spark_round
)
from pyspark.ml.feature import VectorAssembler, StandardScaler
from pyspark.ml.clustering import KMeans
from pyspark.ml.evaluation import ClusteringEvaluator
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def create_spark_session():
    """T·∫°o Spark Session"""
    
    spark = SparkSession.builder \
        .appName("CustomerClustering") \
        .master("spark://spark-master:7077") \
        .config("spark.sql.warehouse.dir", "/user/hive/warehouse") \
        .config("hive.metastore.uris", "thrift://hive-metastore:9083") \
        .enableHiveSupport() \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")
    return spark


def prepare_customer_features(spark):
    """
    Chu·∫©n b·ªã features cho clustering:
    - Recency: S·ªë ng√†y t·ª´ l·∫ßn mua cu·ªëi
    - Frequency: S·ªë l·∫ßn mua h√†ng
    - Monetary: T·ªïng chi ti√™u
    - AvgOrderValue: Gi√° tr·ªã ƒë∆°n h√†ng trung b√¨nh
    - UniqueProducts: S·ªë s·∫£n ph·∫©m unique ƒë√£ mua
    - AvgQuantity: S·ªë l∆∞·ª£ng trung b√¨nh m·ªói ƒë∆°n
    """
    
    logger.info("üìä Preparing customer features for clustering...")
    
    customer_features = spark.sql("""
        SELECT 
            CustomerID,
            Country,
            DATEDIFF(
                (SELECT MAX(InvoiceDate) FROM retail_db.transactions),
                MAX(InvoiceDate)
            ) as Recency,
            COUNT(DISTINCT InvoiceNo) as Frequency,
            ROUND(SUM(TotalAmount), 2) as Monetary,
            ROUND(AVG(TotalAmount), 2) as AvgOrderValue,
            COUNT(DISTINCT StockCode) as UniqueProducts,
            ROUND(AVG(Quantity), 2) as AvgQuantity,
            ROUND(SUM(Quantity), 0) as TotalQuantity
        FROM retail_db.transactions
        GROUP BY CustomerID, Country
        HAVING COUNT(DISTINCT InvoiceNo) >= 2
    """)
    
    logger.info(f"‚úÖ Customer features prepared: {customer_features.count()} customers")
    
    return customer_features


def find_optimal_k(scaled_data, max_k=10):
    """T√¨m s·ªë cluster t·ªëi ∆∞u b·∫±ng Elbow method"""
    
    logger.info("üîç Finding optimal number of clusters...")
    
    costs = []
    evaluator = ClusteringEvaluator()
    
    for k in range(2, max_k + 1):
        kmeans = KMeans().setK(k).setSeed(42).setFeaturesCol("scaled_features")
        model = kmeans.fit(scaled_data)
        
        # Inertia (Within Set Sum of Squared Errors)
        cost = model.summary.trainingCost
        
        # Silhouette score
        predictions = model.transform(scaled_data)
        silhouette = evaluator.evaluate(predictions)
        
        costs.append({
            'k': k,
            'cost': cost,
            'silhouette': silhouette
        })
        
        logger.info(f"  K={k}: Cost={cost:.2f}, Silhouette={silhouette:.4f}")
    
    # T√¨m K t·ªët nh·∫•t d·ª±a tr√™n silhouette score
    best_k = max(costs, key=lambda x: x['silhouette'])['k']
    logger.info(f"‚úÖ Optimal K = {best_k}")
    
    return best_k, costs


def run_kmeans_clustering(spark, customer_features, n_clusters=5):
    """
    Ch·∫°y K-Means Clustering
    """
    
    logger.info(f"üéØ Running K-Means with {n_clusters} clusters...")
    
    # Ch·ªçn c√°c features cho clustering
    feature_cols = ['Recency', 'Frequency', 'Monetary', 'AvgOrderValue', 
                    'UniqueProducts', 'AvgQuantity']
    
    # Vector Assembler
    assembler = VectorAssembler(
        inputCols=feature_cols,
        outputCol="features"
    )
    
    df_vector = assembler.transform(customer_features)
    
    # Chu·∫©n h√≥a features
    scaler = StandardScaler(
        inputCol="features",
        outputCol="scaled_features",
        withStd=True,
        withMean=True
    )
    
    scaler_model = scaler.fit(df_vector)
    df_scaled = scaler_model.transform(df_vector)
    
    # Train K-Means model
    kmeans = KMeans() \
        .setK(n_clusters) \
        .setSeed(42) \
        .setFeaturesCol("scaled_features") \
        .setPredictionCol("Cluster")
    
    model = kmeans.fit(df_scaled)
    
    # Predict clusters
    predictions = model.transform(df_scaled)
    
    # Evaluate
    evaluator = ClusteringEvaluator()
    silhouette = evaluator.evaluate(predictions)
    logger.info(f"üìà Silhouette Score: {silhouette:.4f}")
    
    # T√≠nh cluster centers
    centers = model.clusterCenters()
    logger.info("\nüìç Cluster Centers:")
    for i, center in enumerate(centers):
        logger.info(f"  Cluster {i}: {[round(x, 2) for x in center]}")
    
    return predictions, model


def analyze_clusters(spark, predictions):
    """
    Ph√¢n t√≠ch ƒë·∫∑c ƒëi·ªÉm c·ªßa t·ª´ng cluster
    """
    
    logger.info("üî¨ Analyzing cluster characteristics...")
    
    # T·∫°o view t·∫°m th·ªùi
    predictions.createOrReplaceTempView("clustered_customers")
    
    # Th·ªëng k√™ theo cluster
    cluster_stats = spark.sql("""
        SELECT 
            Cluster,
            COUNT(*) as CustomerCount,
            ROUND(AVG(Recency), 1) as AvgRecency,
            ROUND(AVG(Frequency), 1) as AvgFrequency,
            ROUND(AVG(Monetary), 2) as AvgMonetary,
            ROUND(AVG(AvgOrderValue), 2) as AvgOrderValue,
            ROUND(AVG(UniqueProducts), 1) as AvgUniqueProducts,
            ROUND(SUM(Monetary), 2) as TotalRevenue
        FROM clustered_customers
        GROUP BY Cluster
        ORDER BY AvgMonetary DESC
    """)
    
    # G√°n nh√£n cho clusters d·ª±a tr√™n ƒë·∫∑c ƒëi·ªÉm
    cluster_labels = spark.sql("""
        SELECT 
            Cluster,
            CASE 
                WHEN AvgRecency <= 30 AND AvgFrequency >= 10 AND AvgMonetary >= 1000 
                    THEN 'VIP Customers'
                WHEN AvgRecency <= 60 AND AvgFrequency >= 5 AND AvgMonetary >= 500 
                    THEN 'Loyal Customers'
                WHEN AvgRecency <= 30 AND AvgFrequency <= 3 
                    THEN 'New Customers'
                WHEN AvgRecency >= 90 AND AvgFrequency <= 2 
                    THEN 'Lost Customers'
                WHEN AvgRecency >= 60 AND AvgMonetary >= 300 
                    THEN 'At Risk'
                ELSE 'Regular Customers'
            END as ClusterLabel,
            CustomerCount,
            AvgRecency,
            AvgFrequency,
            AvgMonetary,
            TotalRevenue
        FROM (
            SELECT 
                Cluster,
                COUNT(*) as CustomerCount,
                AVG(Recency) as AvgRecency,
                AVG(Frequency) as AvgFrequency,
                AVG(Monetary) as AvgMonetary,
                SUM(Monetary) as TotalRevenue
            FROM clustered_customers
            GROUP BY Cluster
        ) stats
        ORDER BY AvgMonetary DESC
    """)
    
    cluster_stats.show()
    cluster_labels.show()
    
    # L∆∞u k·∫øt qu·∫£ v√†o Hive
    result = predictions.select(
        "CustomerID", "Country", "Recency", "Frequency", 
        "Monetary", "AvgOrderValue", "UniqueProducts", "Cluster"
    )
    
    result.write.mode("overwrite").saveAsTable("retail_db.customer_clusters")
    cluster_stats.write.mode("overwrite").saveAsTable("retail_db.cluster_statistics")
    
    logger.info("‚úÖ Cluster analysis saved to Hive")
    
    return cluster_stats


def save_to_mongodb(predictions):
    """L∆∞u k·∫øt qu·∫£ clustering v√†o MongoDB"""
    
    logger.info("üì§ Saving clustering results to MongoDB...")
    
    result = predictions.select(
        "CustomerID", "Country", "Recency", "Frequency", 
        "Monetary", "AvgOrderValue", "UniqueProducts", "Cluster"
    )
    
    result.write \
        .format("mongo") \
        .mode("overwrite") \
        .option("uri", "mongodb://admin:admin123@mongodb:27017/retail_analytics.customer_clusters?authSource=admin") \
        .save()
    
    logger.info("‚úÖ Clustering results saved to MongoDB")


def main():
    """Main function"""
    
    logger.info("üöÄ Starting Customer Clustering Analysis...")
    logger.info("=" * 60)
    
    spark = create_spark_session()
    
    try:
        # 1. Chu·∫©n b·ªã features
        customer_features = prepare_customer_features(spark)
        
        # 2. Ch·∫°y K-Means
        predictions, model = run_kmeans_clustering(spark, customer_features, n_clusters=5)
        
        # 3. Ph√¢n t√≠ch clusters
        cluster_stats = analyze_clusters(spark, predictions)
        
        # 4. L∆∞u v√†o MongoDB
        try:
            save_to_mongodb(predictions)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Could not save to MongoDB: {e}")
        
        logger.info("=" * 60)
        logger.info("‚úÖ Customer Clustering completed successfully!")
        
    except Exception as e:
        logger.error(f"‚ùå Clustering failed: {e}")
        raise
    finally:
        spark.stop()


if __name__ == "__main__":
    main()
