"""
Retail Data ETL Pipeline - Main Spark Application
===================================================
Pipeline x·ª≠ l√Ω d·ªØ li·ªáu b√°n l·∫ª v·ªõi c√°c ch·ª©c nƒÉng:
1. Load d·ªØ li·ªáu t·ª´ CSV v√†o HDFS
2. X·ª≠ l√Ω v√† l√†m s·∫°ch d·ªØ li·ªáu
3. T·∫°o c√°c b·∫£ng ph√¢n t√≠ch tr√™n Hive
4. L∆∞u k·∫øt qu·∫£ ph√¢n t√≠ch v√†o MongoDB
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, sum as spark_sum, count, avg, 
    month, year, dayofweek, hour, 
    when, lit, round as spark_round,
    desc, asc, to_date, to_timestamp,
    regexp_replace, trim, upper,
    row_number, dense_rank
)
from pyspark.sql.window import Window
from pyspark.sql.types import (
    StructType, StructField, StringType, 
    IntegerType, DoubleType, TimestampType
)
import logging

# C·∫•u h√¨nh logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def create_spark_session():
    """T·∫°o Spark Session v·ªõi c·∫•u h√¨nh k·∫øt n·ªëi Hive v√† MongoDB"""
    
    spark = SparkSession.builder \
        .appName("RetailDataPipeline") \
        .master("spark://spark-master:7077") \
        .config("spark.sql.warehouse.dir", "hdfs://namenode:9000/user/hive/warehouse") \
        .config("hive.metastore.uris", "thrift://hive-metastore:9083") \
        .config("spark.hadoop.hive.metastore.warehouse.dir", "hdfs://namenode:9000/user/hive/warehouse") \
        .config("spark.mongodb.input.uri", "mongodb://admin:admin123@mongodb:27017/retail_analytics.transactions?authSource=admin") \
        .config("spark.mongodb.output.uri", "mongodb://admin:admin123@mongodb:27017/retail_analytics.transactions?authSource=admin") \
        .config("spark.jars.packages", "org.mongodb.spark:mongo-spark-connector_2.12:3.0.1") \
        .config("spark.hadoop.hive.metastore.client.socket.timeout", "1800s") \
        .config("spark.sql.hive.metastore.sharedPrefixes", "org.postgresql") \
        .config("spark.sql.broadcastTimeout", "1800") \
        .config("spark.network.timeout", "1800s") \
        .config("spark.executor.heartbeatInterval", "300s") \
        .enableHiveSupport() \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")
    logger.info("‚úÖ Spark Session created successfully")
    return spark


def define_schema():
    """ƒê·ªãnh nghƒ©a schema cho d·ªØ li·ªáu b√°n l·∫ª"""
    
    return StructType([
        StructField("InvoiceNo", StringType(), True),
        StructField("StockCode", StringType(), True),
        StructField("Description", StringType(), True),
        StructField("Quantity", IntegerType(), True),
        StructField("InvoiceDate", StringType(), True),
        StructField("UnitPrice", DoubleType(), True),
        StructField("CustomerID", DoubleType(), True),
        StructField("Country", StringType(), True)
    ])


def load_and_clean_data(spark, input_path):
    """
    Load v√† l√†m s·∫°ch d·ªØ li·ªáu t·ª´ CSV
    - Lo·∫°i b·ªè gi√° tr·ªã null
    - Lo·∫°i b·ªè ƒë∆°n h√†ng h·ªßy (InvoiceNo b·∫Øt ƒë·∫ßu b·∫±ng 'C')
    - Chuy·ªÉn ƒë·ªïi ki·ªÉu d·ªØ li·ªáu
    - T√≠nh TotalAmount
    """
    
    logger.info(f"üìÇ Loading data from: {input_path}")
    
    schema = define_schema()
    
    # Load d·ªØ li·ªáu
    df = spark.read \
        .option("header", "true") \
        .option("inferSchema", "false") \
        .schema(schema) \
        .csv(input_path)
    
    logger.info(f"üìä Raw records: {df.count()}")
    
    # L√†m s·∫°ch d·ªØ li·ªáu
    df_cleaned = df \
        .filter(col("CustomerID").isNotNull()) \
        .filter(col("Quantity") > 0) \
        .filter(col("UnitPrice") > 0) \
        .filter(~col("InvoiceNo").startswith("C")) \
        .withColumn("InvoiceDate", to_timestamp(col("InvoiceDate"), "yyyy-MM-dd HH:mm:ss")) \
        .withColumn("CustomerID", col("CustomerID").cast(IntegerType())) \
        .withColumn("TotalAmount", spark_round(col("Quantity") * col("UnitPrice"), 2)) \
        .withColumn("Description", trim(upper(col("Description")))) \
        .withColumn("Year", year(col("InvoiceDate"))) \
        .withColumn("Month", month(col("InvoiceDate"))) \
        .withColumn("DayOfWeek", dayofweek(col("InvoiceDate"))) \
        .withColumn("Hour", hour(col("InvoiceDate")))
    
    logger.info(f"‚úÖ Cleaned records: {df_cleaned.count()}")
    
    return df_cleaned


def save_to_hdfs(df, path, format="parquet"):
    """L∆∞u DataFrame v√†o HDFS"""
    
    logger.info(f"üíæ Saving data to HDFS: {path}")
    
    df.write \
        .mode("overwrite") \
        .format(format) \
        .save(path)
    
    logger.info(f"‚úÖ Data saved to HDFS successfully")


def create_hive_tables(spark, df):
    """T·∫°o c√°c b·∫£ng Hive t·ª´ d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω - s·ª≠ d·ª•ng External Table ƒë·ªÉ tr√°nh timeout"""
    
    logger.info("üèóÔ∏è Creating Hive database and tables...")
    
    # ƒê∆∞·ªùng d·∫´n HDFS cho d·ªØ li·ªáu
    hdfs_path = "hdfs://namenode:9000/user/retail/transactions_data"
    
    # B∆∞·ªõc 1: L∆∞u d·ªØ li·ªáu v√†o HDFS d·∫°ng Parquet (kh√¥ng c·∫ßn Hive)
    logger.info(f"üíæ Saving data to HDFS: {hdfs_path}")
    df.write \
        .mode("overwrite") \
        .format("parquet") \
        .save(hdfs_path)
    logger.info("‚úÖ Data saved to HDFS successfully")
    
    # B∆∞·ªõc 2: T·∫°o database (ƒë∆°n gi·∫£n, √≠t timeout)
    logger.info("üìÅ Creating database...")
    spark.sql("CREATE DATABASE IF NOT EXISTS retail_db")
    spark.sql("USE retail_db")
    
    # B∆∞·ªõc 3: Drop table c≈© n·∫øu c√≥
    spark.sql("DROP TABLE IF EXISTS retail_db.transactions")
    logger.info("üóëÔ∏è Dropped existing table if any")
    
    # B∆∞·ªõc 4: T·∫°o EXTERNAL TABLE tr·ªè ƒë·∫øn d·ªØ li·ªáu ƒë√£ l∆∞u
    # External table ch·ªâ t·∫°o metadata, kh√¥ng copy d·ªØ li·ªáu -> nhanh, kh√¥ng timeout
    logger.info("üìã Creating external table...")
    
    create_table_sql = f"""
        CREATE EXTERNAL TABLE IF NOT EXISTS retail_db.transactions (
            InvoiceNo STRING,
            StockCode STRING,
            Description STRING,
            Quantity INT,
            InvoiceDate TIMESTAMP,
            UnitPrice DOUBLE,
            CustomerID INT,
            Country STRING,
            TotalAmount DOUBLE,
            Year INT,
            Month INT,
            DayOfWeek INT,
            Hour INT
        )
        STORED AS PARQUET
        LOCATION '{hdfs_path}'
    """
    
    spark.sql(create_table_sql)
    logger.info("‚úÖ External table retail_db.transactions created")
    
    # B∆∞·ªõc 5: ƒêƒÉng k√Ω df nh∆∞ temporary view ƒë·ªÉ d√πng trong ph√¢n t√≠ch
    df.createOrReplaceTempView("transactions")
    logger.info("‚úÖ Temporary view 'transactions' created for analysis")
    
    return True


def analyze_revenue_by_time(spark):
    """
    Ph√¢n t√≠ch doanh thu theo th·ªùi gian:
    - Theo th√°ng
    - Theo ng√†y trong tu·∫ßn
    - Theo gi·ªù
    """
    
    logger.info("üìà Analyzing revenue by time...")
    
    # Doanh thu theo th√°ng (d√πng temp view 'transactions')
    monthly_revenue = spark.sql("""
        SELECT 
            Year,
            Month,
            COUNT(DISTINCT InvoiceNo) as TotalOrders,
            COUNT(DISTINCT CustomerID) as TotalCustomers,
            SUM(Quantity) as TotalQuantity,
            ROUND(SUM(TotalAmount), 2) as TotalRevenue,
            ROUND(AVG(TotalAmount), 2) as AvgOrderValue
        FROM transactions
        GROUP BY Year, Month
        ORDER BY Year, Month
    """)
    
    # Doanh thu theo ng√†y trong tu·∫ßn
    daily_revenue = spark.sql("""
        SELECT 
            DayOfWeek,
            CASE DayOfWeek
                WHEN 1 THEN 'Sunday'
                WHEN 2 THEN 'Monday'
                WHEN 3 THEN 'Tuesday'
                WHEN 4 THEN 'Wednesday'
                WHEN 5 THEN 'Thursday'
                WHEN 6 THEN 'Friday'
                WHEN 7 THEN 'Saturday'
            END as DayName,
            COUNT(DISTINCT InvoiceNo) as TotalOrders,
            ROUND(SUM(TotalAmount), 2) as TotalRevenue
        FROM transactions
        GROUP BY DayOfWeek
        ORDER BY DayOfWeek
    """)
    
    # Doanh thu theo gi·ªù
    hourly_revenue = spark.sql("""
        SELECT 
            Hour,
            COUNT(DISTINCT InvoiceNo) as TotalOrders,
            ROUND(SUM(TotalAmount), 2) as TotalRevenue
        FROM transactions
        GROUP BY Hour
        ORDER BY Hour
    """)
    
    # L∆∞u v√†o HDFS thay v√¨ Hive (tr√°nh timeout)
    hdfs_base = "hdfs://namenode:9000/user/retail/analysis"
    monthly_revenue.write.mode("overwrite").parquet(f"{hdfs_base}/monthly_revenue")
    daily_revenue.write.mode("overwrite").parquet(f"{hdfs_base}/daily_revenue")
    hourly_revenue.write.mode("overwrite").parquet(f"{hdfs_base}/hourly_revenue")
    
    # T·∫°o temp views cho c√°c b∆∞·ªõc sau
    monthly_revenue.createOrReplaceTempView("monthly_revenue")
    daily_revenue.createOrReplaceTempView("daily_revenue")
    hourly_revenue.createOrReplaceTempView("hourly_revenue")
    
    logger.info("‚úÖ Revenue analysis completed and saved to HDFS")
    
    return monthly_revenue, daily_revenue, hourly_revenue


def analyze_top_products(spark, top_n=20):
    """
    Ph√¢n t√≠ch s·∫£n ph·∫©m b√°n ch·∫°y:
    - Top s·∫£n ph·∫©m theo s·ªë l∆∞·ª£ng
    - Top s·∫£n ph·∫©m theo doanh thu
    """
    
    logger.info(f"üèÜ Analyzing top {top_n} products...")
    
    # Top s·∫£n ph·∫©m theo s·ªë l∆∞·ª£ng
    top_by_quantity = spark.sql(f"""
        SELECT 
            StockCode,
            Description,
            SUM(Quantity) as TotalQuantity,
            COUNT(DISTINCT InvoiceNo) as TotalOrders,
            COUNT(DISTINCT CustomerID) as TotalCustomers,
            ROUND(SUM(TotalAmount), 2) as TotalRevenue
        FROM transactions
        GROUP BY StockCode, Description
        ORDER BY TotalQuantity DESC
        LIMIT {top_n}
    """)
    
    # Top s·∫£n ph·∫©m theo doanh thu
    top_by_revenue = spark.sql(f"""
        SELECT 
            StockCode,
            Description,
            ROUND(SUM(TotalAmount), 2) as TotalRevenue,
            SUM(Quantity) as TotalQuantity,
            COUNT(DISTINCT InvoiceNo) as TotalOrders,
            ROUND(AVG(UnitPrice), 2) as AvgPrice
        FROM transactions
        GROUP BY StockCode, Description
        ORDER BY TotalRevenue DESC
        LIMIT {top_n}
    """)
    
    # L∆∞u v√†o HDFS
    hdfs_base = "hdfs://namenode:9000/user/retail/analysis"
    top_by_quantity.write.mode("overwrite").parquet(f"{hdfs_base}/top_products_by_quantity")
    top_by_revenue.write.mode("overwrite").parquet(f"{hdfs_base}/top_products_by_revenue")
    
    # T·∫°o temp views
    top_by_quantity.createOrReplaceTempView("top_products_by_quantity")
    top_by_revenue.createOrReplaceTempView("top_products_by_revenue")
    
    logger.info("‚úÖ Top products analysis completed and saved to HDFS")
    
    return top_by_quantity, top_by_revenue


def analyze_customer_behavior(spark):
    """
    Ph√¢n t√≠ch h√†nh vi kh√°ch h√†ng:
    - RFM Analysis (Recency, Frequency, Monetary)
    - Ph√¢n kh√∫c kh√°ch h√†ng
    """
    
    logger.info("üë• Analyzing customer behavior...")
    
    # T√≠nh RFM cho t·ª´ng kh√°ch h√†ng
    rfm_analysis = spark.sql("""
        WITH customer_stats AS (
            SELECT 
                CustomerID,
                Country,
                DATEDIFF(
                    (SELECT MAX(InvoiceDate) FROM transactions),
                    MAX(InvoiceDate)
                ) as Recency,
                COUNT(DISTINCT InvoiceNo) as Frequency,
                ROUND(SUM(TotalAmount), 2) as Monetary,
                MIN(InvoiceDate) as FirstPurchase,
                MAX(InvoiceDate) as LastPurchase
            FROM transactions
            GROUP BY CustomerID, Country
        ),
        rfm_scores AS (
            SELECT 
                *,
                NTILE(5) OVER (ORDER BY Recency DESC) as R_Score,
                NTILE(5) OVER (ORDER BY Frequency) as F_Score,
                NTILE(5) OVER (ORDER BY Monetary) as M_Score
            FROM customer_stats
        )
        SELECT 
            *,
            (R_Score + F_Score + M_Score) as RFM_Score,
            CONCAT(R_Score, F_Score, M_Score) as RFM_Segment,
            CASE 
                WHEN R_Score >= 4 AND F_Score >= 4 AND M_Score >= 4 THEN 'Champions'
                WHEN R_Score >= 3 AND F_Score >= 3 AND M_Score >= 3 THEN 'Loyal Customers'
                WHEN R_Score >= 4 AND F_Score <= 2 THEN 'New Customers'
                WHEN R_Score <= 2 AND F_Score >= 3 THEN 'At Risk'
                WHEN R_Score <= 2 AND F_Score <= 2 AND M_Score <= 2 THEN 'Lost'
                ELSE 'Regular'
            END as CustomerSegment
        FROM rfm_scores
    """)
    
    # Th·ªëng k√™ theo ph√¢n kh√∫c
    segment_stats = spark.sql("""
        SELECT 
            CustomerSegment,
            COUNT(*) as CustomerCount,
            ROUND(AVG(Monetary), 2) as AvgMonetary,
            ROUND(AVG(Frequency), 2) as AvgFrequency,
            ROUND(AVG(Recency), 2) as AvgRecency
        FROM (
            SELECT 
                CustomerID,
                CASE 
                    WHEN R_Score >= 4 AND F_Score >= 4 AND M_Score >= 4 THEN 'Champions'
                    WHEN R_Score >= 3 AND F_Score >= 3 AND M_Score >= 3 THEN 'Loyal Customers'
                    WHEN R_Score >= 4 AND F_Score <= 2 THEN 'New Customers'
                    WHEN R_Score <= 2 AND F_Score >= 3 THEN 'At Risk'
                    WHEN R_Score <= 2 AND F_Score <= 2 AND M_Score <= 2 THEN 'Lost'
                    ELSE 'Regular'
                END as CustomerSegment,
                Monetary, Frequency, Recency, R_Score, F_Score, M_Score
            FROM (
                SELECT 
                    CustomerID,
                    DATEDIFF(
                        (SELECT MAX(InvoiceDate) FROM transactions),
                        MAX(InvoiceDate)
                    ) as Recency,
                    COUNT(DISTINCT InvoiceNo) as Frequency,
                    SUM(TotalAmount) as Monetary,
                    NTILE(5) OVER (ORDER BY DATEDIFF(
                        (SELECT MAX(InvoiceDate) FROM transactions),
                        MAX(InvoiceDate)
                    ) DESC) as R_Score,
                    NTILE(5) OVER (ORDER BY COUNT(DISTINCT InvoiceNo)) as F_Score,
                    NTILE(5) OVER (ORDER BY SUM(TotalAmount)) as M_Score
                FROM transactions
                GROUP BY CustomerID
            ) rfm_base
        ) segmented
        GROUP BY CustomerSegment
        ORDER BY CustomerCount DESC
    """)
    
    # L∆∞u v√†o HDFS
    hdfs_base = "hdfs://namenode:9000/user/retail/analysis"
    rfm_analysis.write.mode("overwrite").parquet(f"{hdfs_base}/customer_rfm")
    segment_stats.write.mode("overwrite").parquet(f"{hdfs_base}/customer_segments")
    
    # T·∫°o temp views
    rfm_analysis.createOrReplaceTempView("customer_rfm")
    segment_stats.createOrReplaceTempView("customer_segments")
    
    logger.info("‚úÖ Customer behavior analysis completed and saved to HDFS")
    
    return rfm_analysis, segment_stats


def analyze_country_performance(spark):
    """Ph√¢n t√≠ch hi·ªáu su·∫•t theo qu·ªëc gia"""
    
    logger.info("üåç Analyzing country performance...")
    
    country_stats = spark.sql("""
        SELECT 
            Country,
            COUNT(DISTINCT CustomerID) as TotalCustomers,
            COUNT(DISTINCT InvoiceNo) as TotalOrders,
            SUM(Quantity) as TotalQuantity,
            ROUND(SUM(TotalAmount), 2) as TotalRevenue,
            ROUND(AVG(TotalAmount), 2) as AvgOrderValue,
            ROUND(SUM(TotalAmount) / COUNT(DISTINCT CustomerID), 2) as RevenuePerCustomer
        FROM transactions
        GROUP BY Country
        ORDER BY TotalRevenue DESC
    """)
    
    # L∆∞u v√†o HDFS
    hdfs_base = "hdfs://namenode:9000/user/retail/analysis"
    country_stats.write.mode("overwrite").parquet(f"{hdfs_base}/country_performance")
    
    # T·∫°o temp view
    country_stats.createOrReplaceTempView("country_performance")
    
    logger.info("‚úÖ Country performance analysis completed and saved to HDFS")
    
    return country_stats


def analyze_purchase_patterns(spark):
    """Ph√¢n t√≠ch xu h∆∞·ªõng mua h√†ng"""
    
    logger.info("üìä Analyzing purchase patterns...")
    
    # S·∫£n ph·∫©m th∆∞·ªùng ƒë∆∞·ª£c mua c√πng nhau
    basket_analysis = spark.sql("""
        SELECT 
            a.StockCode as Product1,
            b.StockCode as Product2,
            COUNT(*) as Frequency
        FROM transactions a
        JOIN transactions b 
            ON a.InvoiceNo = b.InvoiceNo 
            AND a.StockCode < b.StockCode
        GROUP BY a.StockCode, b.StockCode
        HAVING COUNT(*) > 50
        ORDER BY Frequency DESC
        LIMIT 100
    """)
    
    # Xu h∆∞·ªõng mua h√†ng theo th√°ng
    monthly_trend = spark.sql("""
        SELECT 
            Year,
            Month,
            COUNT(DISTINCT CustomerID) as UniqueCustomers,
            COUNT(DISTINCT InvoiceNo) as TotalOrders,
            ROUND(SUM(TotalAmount), 2) as TotalRevenue,
            ROUND(SUM(TotalAmount) / COUNT(DISTINCT InvoiceNo), 2) as AvgOrderValue,
            LAG(SUM(TotalAmount)) OVER (ORDER BY Year, Month) as PrevMonthRevenue
        FROM transactions
        GROUP BY Year, Month
        ORDER BY Year, Month
    """)
    
    # L∆∞u v√†o HDFS
    hdfs_base = "hdfs://namenode:9000/user/retail/analysis"
    basket_analysis.write.mode("overwrite").parquet(f"{hdfs_base}/basket_analysis")
    monthly_trend.write.mode("overwrite").parquet(f"{hdfs_base}/monthly_trend")
    
    # T·∫°o temp views
    basket_analysis.createOrReplaceTempView("basket_analysis")
    monthly_trend.createOrReplaceTempView("monthly_trend")
    
    logger.info("‚úÖ Purchase patterns analysis completed and saved to HDFS")
    
    return basket_analysis, monthly_trend


def save_to_mongodb(spark, view_name, collection_name):
    """L∆∞u k·∫øt qu·∫£ ph√¢n t√≠ch v√†o MongoDB t·ª´ temp view"""
    
    logger.info(f"üì§ Saving {view_name} to MongoDB collection: {collection_name}")
    
    try:
        df = spark.sql(f"SELECT * FROM {view_name}")
        
        df.write \
            .format("mongo") \
            .mode("overwrite") \
            .option("uri", f"mongodb://admin:admin123@mongodb:27017/retail_analytics.{collection_name}?authSource=admin") \
            .save()
        
        logger.info(f"‚úÖ Data saved to MongoDB: {collection_name}")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Could not save {view_name} to MongoDB: {e}")


def run_pipeline():
    """Ch·∫°y to√†n b·ªô pipeline ETL"""
    
    logger.info("üöÄ Starting Retail Data Pipeline...")
    logger.info("=" * 60)
    
    # 1. T·∫°o Spark Session
    spark = create_spark_session()
    
    try:
        # 2. Load v√† l√†m s·∫°ch d·ªØ li·ªáu
        input_path = "/data/online_retail.csv"
        df = load_and_clean_data(spark, input_path)
        
        # 3. L∆∞u d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω v√†o HDFS
        save_to_hdfs(df, "hdfs://namenode:9000/user/retail/processed_data")
        
        # 4. T·∫°o b·∫£ng Hive (External Table)
        create_hive_tables(spark, df)
        
        # 5. Ch·∫°y c√°c ph√¢n t√≠ch
        logger.info("=" * 60)
        logger.info("üîç Running Analytics...")
        
        analyze_revenue_by_time(spark)
        analyze_top_products(spark)
        analyze_customer_behavior(spark)
        analyze_country_performance(spark)
        analyze_purchase_patterns(spark)
        
        # 6. L∆∞u k·∫øt qu·∫£ v√†o MongoDB (d√πng temp views)
        logger.info("=" * 60)
        logger.info("üíæ Saving results to MongoDB...")
        
        # Danh s√°ch temp views ƒë·ªÉ l∆∞u v√†o MongoDB
        views_to_mongo = [
            ("monthly_revenue", "monthly_revenue"),
            ("daily_revenue", "daily_revenue"),
            ("hourly_revenue", "hourly_revenue"),
            ("top_products_by_quantity", "top_products_quantity"),
            ("top_products_by_revenue", "top_products_revenue"),
            ("customer_rfm", "customer_rfm"),
            ("customer_segments", "customer_segments"),
            ("country_performance", "country_performance"),
            ("monthly_trend", "monthly_trend")
        ]
        
        for view, collection in views_to_mongo:
            save_to_mongodb(spark, view, collection)
        
        logger.info("=" * 60)
        logger.info("‚úÖ Pipeline completed successfully!")
        logger.info("=" * 60)
        
        # Hi·ªÉn th·ªã t√≥m t·∫Øt (d√πng temp view 'transactions')
        logger.info("\nüìã SUMMARY:")
        logger.info("-" * 40)
        
        total_records = spark.sql("SELECT COUNT(*) as cnt FROM transactions").collect()[0]['cnt']
        total_customers = spark.sql("SELECT COUNT(DISTINCT CustomerID) as cnt FROM transactions").collect()[0]['cnt']
        total_revenue = spark.sql("SELECT ROUND(SUM(TotalAmount), 2) as total FROM transactions").collect()[0]['total']
        
        logger.info(f"üìä Total Records: {total_records:,}")
        logger.info(f"üë• Total Customers: {total_customers:,}")
        logger.info(f"üí∞ Total Revenue: ¬£{total_revenue:,.2f}")
        
        # Hi·ªÉn th·ªã ƒë∆∞·ªùng d·∫´n HDFS
        logger.info("\nüìÅ DATA LOCATIONS:")
        logger.info("-" * 40)
        logger.info("üìÇ Raw transactions: hdfs://namenode:9000/user/retail/transactions_data")
        logger.info("üìÇ Analysis results: hdfs://namenode:9000/user/retail/analysis/")
        
    except Exception as e:
        logger.error(f"‚ùå Pipeline failed: {e}")
        raise
    finally:
        spark.stop()
        logger.info("üõë Spark Session stopped")


if __name__ == "__main__":
    run_pipeline()
