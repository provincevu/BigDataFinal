# ğŸ–¥ï¸ HÆ°á»›ng dáº«n Sá»­ dá»¥ng Giao diá»‡n Web - Retail Big Data Pipeline

## ğŸ“‹ Má»¥c lá»¥c

- [Tá»•ng quan cÃ¡c Web UI](#tá»•ng-quan-cÃ¡c-web-ui)
- [1. Hadoop HDFS Web UI](#1-hadoop-hdfs-web-ui)
- [2. Apache Spark Web UI](#2-apache-spark-web-ui)
- [3. Hue - Hadoop User Experience](#3-hue---hadoop-user-experience)
- [4. Jupyter Notebook](#4-jupyter-notebook)
- [5. MongoDB Express](#5-mongodb-express)
- [Workflow thá»±c hÃ nh](#workflow-thá»±c-hÃ nh)

---

## ğŸŒ Tá»•ng quan cÃ¡c Web UI

Sau khi khá»Ÿi Ä‘á»™ng há»‡ thá»‘ng báº±ng `start.bat`, báº¡n cÃ³ thá»ƒ truy cáº­p cÃ¡c giao diá»‡n web sau:

| Service                 | URL                   | MÃ´ táº£                        | Credentials           |
| ----------------------- | --------------------- | ---------------------------- | --------------------- |
| ğŸ“‚ **HDFS NameNode**    | http://localhost:9870 | Quáº£n lÃ½ file system phÃ¢n tÃ¡n | KhÃ´ng cáº§n             |
| ğŸ“¦ **HDFS DataNode**    | http://localhost:9864 | ThÃ´ng tin node lÆ°u trá»¯       | KhÃ´ng cáº§n             |
| âš¡ **Spark Master**     | http://localhost:8080 | Quáº£n lÃ½ Spark cluster        | KhÃ´ng cáº§n             |
| âš¡ **Spark Worker**     | http://localhost:8081 | ThÃ´ng tin worker node        | KhÃ´ng cáº§n             |
| ğŸ¨ **Hue**              | http://localhost:8888 | GUI cho Hadoop/Hive          | Táº¡o tÃ i khoáº£n láº§n Ä‘áº§u |
| ğŸ““ **Jupyter Notebook** | http://localhost:8889 | Interactive Python/Spark     | Token (xem hÆ°á»›ng dáº«n) |
| ğŸƒ **MongoDB Express**  | http://localhost:8082 | Quáº£n lÃ½ MongoDB              | admin / admin123      |

---

## 1. Hadoop HDFS Web UI

### ğŸ”— Truy cáº­p: http://localhost:9870

### MÃ´ táº£

Giao diá»‡n quáº£n lÃ½ Hadoop Distributed File System (HDFS) - há»‡ thá»‘ng file phÃ¢n tÃ¡n.

### CÃ¡c tÃ­nh nÄƒng chÃ­nh

#### ğŸ“Š Overview (Trang chá»§)

- **Cluster Summary**: Tá»•ng quan vá» cluster (dung lÆ°á»£ng, sá»‘ node, tráº¡ng thÃ¡i)
- **NameNode Status**: Tráº¡ng thÃ¡i cá»§a NameNode
- **Capacity Used**: Pháº§n trÄƒm dung lÆ°á»£ng Ä‘Ã£ sá»­ dá»¥ng

![HDFS Overview](docs/hdfs-overview.png)

#### ğŸ“ Utilities > Browse the file system

**ÄÆ°á»ng dáº«n**: Menu **Utilities** â†’ **Browse the file system**

Cho phÃ©p báº¡n:

- Duyá»‡t cáº¥u trÃºc thÆ° má»¥c trÃªn HDFS
- Xem thÃ´ng tin file (kÃ­ch thÆ°á»›c, quyá»n, thá»i gian táº¡o)
- Download file vá» mÃ¡y local
- Xem ná»™i dung file text

**CÃ¡c thÆ° má»¥c quan trá»ng:**

```
/user/retail/          â†’ Dá»¯ liá»‡u raw (online_retail.csv)
/user/hive/warehouse/  â†’ Hive tables
/user/retail/processed_data/ â†’ Dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½
```

#### ğŸ“ˆ Datanodes

Xem danh sÃ¡ch vÃ  tráº¡ng thÃ¡i cÃ¡c DataNode trong cluster.

### CÃ¡ch sá»­ dá»¥ng

1. **Xem file Ä‘Ã£ upload:**

   - VÃ o **Utilities** â†’ **Browse the file system**
   - Navigate Ä‘áº¿n `/user/retail/`
   - Click vÃ o `online_retail.csv` Ä‘á»ƒ xem thÃ´ng tin

2. **Download file:**
   - TÃ¬m Ä‘áº¿n file cáº§n download
   - Click vÃ o tÃªn file
   - Click nÃºt **Download**

---

## 2. Apache Spark Web UI

### ğŸ”— Truy cáº­p: http://localhost:8080 (Master) | http://localhost:8081 (Worker)

### MÃ´ táº£

Giao diá»‡n quáº£n lÃ½ Apache Spark cluster, theo dÃµi jobs vÃ  resources.

### Spark Master UI (Port 8080)

#### ğŸ“Š Trang chá»§

- **Workers**: Danh sÃ¡ch worker nodes
- **Running Applications**: CÃ¡c á»©ng dá»¥ng Ä‘ang cháº¡y
- **Completed Applications**: CÃ¡c á»©ng dá»¥ng Ä‘Ã£ hoÃ n thÃ nh

#### ğŸ” ThÃ´ng tin hiá»ƒn thá»‹

| Má»¥c           | MÃ´ táº£                                        |
| ------------- | -------------------------------------------- |
| URL           | Spark Master URL (spark://spark-master:7077) |
| Alive Workers | Sá»‘ worker Ä‘ang hoáº¡t Ä‘á»™ng                     |
| Cores in use  | Sá»‘ CPU cores Ä‘ang sá»­ dá»¥ng                    |
| Memory in use | RAM Ä‘ang sá»­ dá»¥ng                             |

### Spark Worker UI (Port 8081)

#### ğŸ“Š ThÃ´ng tin Worker

- **Cores**: Sá»‘ CPU cores cá»§a worker
- **Memory**: RAM available
- **Running Executors**: CÃ¡c executor Ä‘ang cháº¡y

### CÃ¡ch sá»­ dá»¥ng

1. **Theo dÃµi Spark Job:**

   - Cháº¡y ETL pipeline: `run-etl.bat`
   - Truy cáº­p http://localhost:8080
   - Xem job trong **Running Applications**
   - Click vÃ o Application ID Ä‘á»ƒ xem chi tiáº¿t

2. **Xem Spark Application UI:**
   - Khi cÃ³ job Ä‘ang cháº¡y, truy cáº­p http://localhost:4040
   - Xem **Jobs**, **Stages**, **Storage**, **Environment**

---

## 3. Hue - Hadoop User Experience

### ğŸ”— Truy cáº­p: http://localhost:8888

### MÃ´ táº£

Giao diá»‡n web toÃ n diá»‡n Ä‘á»ƒ tÆ°Æ¡ng tÃ¡c vá»›i Hadoop ecosystem (HDFS, Hive, Spark).

### Thiáº¿t láº­p láº§n Ä‘áº§u

1. Truy cáº­p http://localhost:8888
2. Táº¡o tÃ i khoáº£n admin:
   - **Username**: `admin`
   - **Password**: `admin123` (hoáº·c tÃ¹y chá»n)
3. Click **Create Account**

### CÃ¡c tÃ­nh nÄƒng chÃ­nh

#### ğŸ“ Editor (Query Editor)

**ÄÆ°á»ng dáº«n**: Menu trÃ¡i â†’ **Editor** â†’ **Hive**

Viáº¿t vÃ  thá»±c thi Hive SQL queries:

```sql
-- Xem danh sÃ¡ch databases
SHOW DATABASES;

-- Sá»­ dá»¥ng database retail
USE retail_db;

-- Xem danh sÃ¡ch tables
SHOW TABLES;

-- Query dá»¯ liá»‡u
SELECT * FROM transactions LIMIT 10;

-- PhÃ¢n tÃ­ch doanh thu theo thÃ¡ng
SELECT
    Year, Month,
    SUM(TotalAmount) as Revenue
FROM transactions
GROUP BY Year, Month
ORDER BY Year, Month;
```

#### ğŸ“ File Browser

**ÄÆ°á»ng dáº«n**: Menu trÃ¡i â†’ **Files**

- Duyá»‡t HDFS file system
- Upload/Download files
- Táº¡o/XÃ³a thÆ° má»¥c
- Xem ná»™i dung file

**CÃ¡c thao tÃ¡c:**

1. Click **New** â†’ **Directory** Ä‘á»ƒ táº¡o thÆ° má»¥c
2. Click **Upload** Ä‘á»ƒ upload file tá»« mÃ¡y local
3. Right-click file â†’ **Download** Ä‘á»ƒ táº£i vá»

#### ğŸ“Š Table Browser (Metastore)

**ÄÆ°á»ng dáº«n**: Menu trÃ¡i â†’ **Tables**

- Xem danh sÃ¡ch databases vÃ  tables
- Xem schema cá»§a tables
- Preview dá»¯ liá»‡u
- Táº¡o table má»›i

#### ğŸ“ˆ Dashboard

Táº¡o cÃ¡c dashboard visualization tá»« query results.

### CÃ¡ch sá»­ dá»¥ng Hue

#### BÆ°á»›c 1: Kiá»ƒm tra káº¿t ná»‘i Hive

```sql
SHOW DATABASES;
```

#### BÆ°á»›c 2: Táº¡o database vÃ  table (náº¿u chÆ°a cÃ³)

```sql
CREATE DATABASE IF NOT EXISTS retail_db;
USE retail_db;

-- Xem cÃ¡c tables Ä‘Ã£ táº¡o bá»Ÿi ETL
SHOW TABLES;
```

#### BÆ°á»›c 3: Query phÃ¢n tÃ­ch

```sql
-- Top 10 sáº£n pháº©m bÃ¡n cháº¡y
SELECT * FROM top_products_by_revenue LIMIT 10;

-- PhÃ¢n khÃºc khÃ¡ch hÃ ng
SELECT * FROM customer_segments;

-- Doanh thu theo thÃ¡ng
SELECT * FROM monthly_revenue ORDER BY Year, Month;
```

### âš ï¸ LÆ°u Ã½

Náº¿u gáº·p lá»—i káº¿t ná»‘i Hive, hÃ£y sá»­ dá»¥ng Jupyter Notebook thay tháº¿.

---

## 4. Jupyter Notebook

### ğŸ”— Truy cáº­p: http://localhost:8889

### Láº¥y Token Ä‘Äƒng nháº­p

**Windows (PowerShell):**

```powershell
docker logs jupyter 2>&1 | Select-String -Pattern "token"
```

**Linux/Mac:**

```bash
docker logs jupyter 2>&1 | grep token
```

Token cÃ³ dáº¡ng: `http://127.0.0.1:8888/lab?token=abc123xyz...`

Copy pháº§n sau `token=` vÃ  paste vÃ o Ã´ nháº­p token.

### MÃ´ táº£

Jupyter Notebook vá»›i PySpark Ä‘á»ƒ phÃ¢n tÃ­ch dá»¯ liá»‡u interactive.

### Giao diá»‡n JupyterLab

#### ğŸ“ File Browser (BÃªn trÃ¡i)

- `work/` - ThÆ° má»¥c lÃ m viá»‡c chÃ­nh
- `data/` - Chá»©a file dá»¯ liá»‡u
- `spark-apps/` - Spark applications

#### ğŸ““ Notebook

File `retail_analysis.ipynb` Ä‘Ã£ Ä‘Æ°á»£c táº¡o sáºµn vá»›i cÃ¡c phÃ¢n tÃ­ch:

1. **Load vÃ  KhÃ¡m phÃ¡ Dá»¯ liá»‡u**
2. **LÃ m sáº¡ch Dá»¯ liá»‡u**
3. **PhÃ¢n tÃ­ch Doanh thu theo Thá»i gian**
4. **Top Sáº£n pháº©m BÃ¡n cháº¡y**
5. **PhÃ¢n tÃ­ch RFM**
6. **Customer Clustering**
7. **PhÃ¢n tÃ­ch theo Quá»‘c gia**

### CÃ¡ch sá»­ dá»¥ng Jupyter

#### BÆ°á»›c 1: Má»Ÿ Notebook

1. Navigate Ä‘áº¿n `work/` trong File Browser
2. Double-click vÃ o `retail_analysis.ipynb`

#### BÆ°á»›c 2: Cháº¡y cells

- **Cháº¡y 1 cell**: `Shift + Enter`
- **Cháº¡y táº¥t cáº£**: Menu **Run** â†’ **Run All Cells**

#### BÆ°á»›c 3: Táº¡o Notebook má»›i

1. Click **File** â†’ **New** â†’ **Notebook**
2. Chá»n kernel **Python 3**

#### Code máº«u - Káº¿t ná»‘i Spark vÃ  Hive

```python
from pyspark.sql import SparkSession

# Táº¡o Spark Session
spark = SparkSession.builder \
    .appName("RetailAnalysis") \
    .master("spark://spark-master:7077") \
    .config("hive.metastore.uris", "thrift://hive-metastore:9083") \
    .enableHiveSupport() \
    .getOrCreate()

# Load dá»¯ liá»‡u tá»« CSV
df = spark.read \
    .option("header", "true") \
    .option("inferSchema", "true") \
    .csv("/home/jovyan/data/online_retail.csv")

# Xem dá»¯ liá»‡u
df.show(10)

# Query Hive tables (sau khi cháº¡y ETL)
spark.sql("SHOW DATABASES").show()
spark.sql("USE retail_db")
spark.sql("SELECT * FROM monthly_revenue").show()
```

### Visualization vá»›i Matplotlib

```python
import matplotlib.pyplot as plt
import pandas as pd

# Chuyá»ƒn Spark DataFrame sang Pandas
monthly_pd = spark.sql("SELECT * FROM monthly_revenue").toPandas()

# Váº½ biá»ƒu Ä‘á»“
plt.figure(figsize=(12, 6))
plt.plot(monthly_pd['Month'], monthly_pd['TotalRevenue'], marker='o')
plt.title('Doanh thu theo thÃ¡ng')
plt.xlabel('ThÃ¡ng')
plt.ylabel('Doanh thu ($)')
plt.grid(True)
plt.show()
```

---

## 5. MongoDB Express

### ğŸ”— Truy cáº­p: http://localhost:8082

### Credentials

- **Username**: `admin`
- **Password**: `admin123`

### MÃ´ táº£

Giao diá»‡n web Ä‘á»ƒ quáº£n lÃ½ MongoDB - nÆ¡i lÆ°u trá»¯ káº¿t quáº£ phÃ¢n tÃ­ch.

### Giao diá»‡n chÃ­nh

#### ğŸ“Š Databases

Danh sÃ¡ch databases trong MongoDB:

- `admin` - System database
- `config` - Configuration
- `local` - Local data
- `retail_analytics` - **Database chÃ­nh cá»§a project**

#### ğŸ“ Collections trong retail_analytics

Sau khi cháº¡y ETL pipeline, cÃ¡c collections sau sáº½ Ä‘Æ°á»£c táº¡o:

| Collection              | MÃ´ táº£                          |
| ----------------------- | ------------------------------ |
| `transactions`          | Dá»¯ liá»‡u giao dá»‹ch Ä‘Ã£ xá»­ lÃ½     |
| `monthly_revenue`       | Doanh thu theo thÃ¡ng           |
| `daily_revenue`         | Doanh thu theo ngÃ y trong tuáº§n |
| `hourly_revenue`        | Doanh thu theo giá»             |
| `top_products_quantity` | Top sáº£n pháº©m theo sá»‘ lÆ°á»£ng     |
| `top_products_revenue`  | Top sáº£n pháº©m theo doanh thu    |
| `customer_rfm`          | PhÃ¢n tÃ­ch RFM khÃ¡ch hÃ ng       |
| `customer_segments`     | PhÃ¢n khÃºc khÃ¡ch hÃ ng           |
| `customer_clusters`     | Káº¿t quáº£ clustering             |
| `country_performance`   | Hiá»‡u suáº¥t theo quá»‘c gia        |
| `monthly_trend`         | Xu hÆ°á»›ng theo thÃ¡ng            |

### CÃ¡ch sá»­ dá»¥ng

#### BÆ°á»›c 1: ÄÄƒng nháº­p

1. Truy cáº­p http://localhost:8082
2. Nháº­p username: `admin`, password: `admin123`

#### BÆ°á»›c 2: Xem dá»¯ liá»‡u

1. Click vÃ o database `retail_analytics`
2. Click vÃ o collection muá»‘n xem (vÃ­ dá»¥: `monthly_revenue`)
3. Xem danh sÃ¡ch documents

#### BÆ°á»›c 3: Query dá»¯ liá»‡u

1. Trong collection, click **New Query**
2. Nháº­p query JSON:

```json
{ "CustomerSegment": "Champions" }
```

3. Click **Find** Ä‘á»ƒ tÃ¬m kiáº¿m

#### BÆ°á»›c 4: Export dá»¯ liá»‡u

1. Chá»n collection
2. Click **Export**
3. Chá»n Ä‘á»‹nh dáº¡ng (JSON/CSV)

### Query MongoDB tá»« Python

```python
from pymongo import MongoClient

# Káº¿t ná»‘i MongoDB
client = MongoClient('mongodb://admin:admin123@localhost:27017/')
db = client['retail_analytics']

# Láº¥y dá»¯ liá»‡u tá»« collection
monthly_revenue = list(db.monthly_revenue.find())
for doc in monthly_revenue:
    print(doc)

# Query vá»›i Ä‘iá»u kiá»‡n
champions = db.customer_rfm.find({"CustomerSegment": "Champions"})
for customer in champions:
    print(customer)
```

---

## ğŸ”„ Workflow thá»±c hÃ nh

### Quy trÃ¬nh hoÃ n chá»‰nh

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. KHá»I Äá»˜NG Há»† THá»NG                                      â”‚
â”‚     > start.bat                                             â”‚
â”‚     Chá» 2-3 phÃºt Ä‘á»ƒ services khá»Ÿi Ä‘á»™ng                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. KIá»‚M TRA HDFS                                           â”‚
â”‚     ğŸŒ http://localhost:9870                                â”‚
â”‚     â†’ Utilities â†’ Browse file system                        â”‚
â”‚     â†’ Kiá»ƒm tra cluster status                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. UPLOAD Dá»® LIá»†U                                          â”‚
â”‚     > upload-data.bat                                       â”‚
â”‚     Hoáº·c: Hue â†’ Files â†’ Upload                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. CHáº Y ETL PIPELINE                                       â”‚
â”‚     > run-etl.bat                                           â”‚
â”‚     ğŸŒ http://localhost:8080 â†’ Theo dÃµi Spark job           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. PHÃ‚N TÃCH Dá»® LIá»†U                                       â”‚
â”‚     Option A: Hue (http://localhost:8888)                   â”‚
â”‚               â†’ Editor â†’ Hive â†’ Viáº¿t SQL queries            â”‚
â”‚                                                             â”‚
â”‚     Option B: Jupyter (http://localhost:8889)               â”‚
â”‚               â†’ Má»Ÿ retail_analysis.ipynb                    â”‚
â”‚               â†’ Cháº¡y cÃ¡c cells phÃ¢n tÃ­ch                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  6. XEM Káº¾T QUáº¢ TRONG MONGODB                               â”‚
â”‚     ğŸŒ http://localhost:8082                                â”‚
â”‚     â†’ retail_analytics â†’ CÃ¡c collections                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### CÃ¡c bÆ°á»›c thá»±c hÃ nh chi tiáº¿t

#### ğŸ¯ BÃ i thá»±c hÃ nh 1: KhÃ¡m phÃ¡ HDFS

1. Truy cáº­p http://localhost:9870
2. Xem **Cluster Summary** - hiá»ƒu cáº¥u trÃºc cluster
3. VÃ o **Utilities** â†’ **Browse file system**
4. Navigate Ä‘áº¿n `/user/retail/` vÃ  xem file `online_retail.csv`

#### ğŸ¯ BÃ i thá»±c hÃ nh 2: Cháº¡y Spark Job

1. Má»Ÿ terminal, cháº¡y `run-etl.bat`
2. Truy cáº­p http://localhost:8080 ngay láº­p tá»©c
3. Quan sÃ¡t job xuáº¥t hiá»‡n trong **Running Applications**
4. Click vÃ o Application ID Ä‘á»ƒ xem chi tiáº¿t stages

#### ğŸ¯ BÃ i thá»±c hÃ nh 3: Query vá»›i Hue

1. Truy cáº­p http://localhost:8888, Ä‘Äƒng nháº­p
2. VÃ o **Editor** â†’ **Hive**
3. Cháº¡y cÃ¡c queries:

```sql
SHOW DATABASES;
USE retail_db;
SHOW TABLES;
SELECT * FROM monthly_revenue;
```

#### ğŸ¯ BÃ i thá»±c hÃ nh 4: PhÃ¢n tÃ­ch vá»›i Jupyter

1. Truy cáº­p http://localhost:8889 (dÃ¹ng token)
2. Má»Ÿ `work/retail_analysis.ipynb`
3. Cháº¡y tá»«ng cell (Shift + Enter)
4. Quan sÃ¡t cÃ¡c biá»ƒu Ä‘á»“ visualization

#### ğŸ¯ BÃ i thá»±c hÃ nh 5: Xem káº¿t quáº£ MongoDB

1. Truy cáº­p http://localhost:8082 (admin/admin123)
2. VÃ o database `retail_analytics`
3. Xem cÃ¡c collections: `customer_segments`, `monthly_revenue`, etc.
4. Thá»­ query: `{ "TotalRevenue": { "$gt": 100000 } }`

---

## ğŸ†˜ Xá»­ lÃ½ sá»± cá»‘

### KhÃ´ng truy cáº­p Ä‘Æ°á»£c Web UI

```powershell
# Kiá»ƒm tra services Ä‘ang cháº¡y
docker-compose ps

# Restart service cá»¥ thá»ƒ
docker-compose restart <service_name>

# Xem logs
docker-compose logs <service_name>
```

### Hue bÃ¡o lá»—i káº¿t ná»‘i Hive

1. Äá»£i 2-3 phÃºt sau khi start
2. Restart Hive services:

```powershell
docker-compose restart hive-metastore hive-server hue
```

3. Sá»­ dá»¥ng Jupyter thay tháº¿

### Jupyter khÃ´ng load Ä‘Æ°á»£c

```powershell
# Láº¥y token má»›i
docker logs jupyter 2>&1 | Select-String -Pattern "token"

# Restart Jupyter
docker-compose restart jupyter
```

### MongoDB Express khÃ´ng hiá»ƒn thá»‹ data

1. Äáº£m báº£o Ä‘Ã£ cháº¡y `run-etl.bat`
2. Refresh trang
3. Kiá»ƒm tra database `retail_analytics` tá»“n táº¡i

---

## ğŸ“š TÃ i liá»‡u bá»• sung

- [Hadoop HDFS Guide](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html)
- [Spark Web UI Guide](https://spark.apache.org/docs/latest/web-ui.html)
- [Hue User Guide](https://docs.gethue.com/)
- [Jupyter Documentation](https://jupyter.org/documentation)
- [MongoDB Manual](https://www.mongodb.com/docs/manual/)

---

**Happy Learning Big Data! ğŸš€**
