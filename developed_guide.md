# HÆ¯á»šNG DáºªN PHÃT TRIá»‚N Dá»° ÃN - Big Data Retail Analytics

## ğŸ“ CÃC FILE KHÃ”NG Sá»¬ Dá»¤NG / CÃ“ THá»‚ XÃ“A

CÃ¡c file sau Ä‘Ã¢y khÃ´ng Ä‘Æ°á»£c sá»­ dá»¥ng trong quy trÃ¬nh chÃ­nh hoáº·c Ä‘Ã£ bá»‹ thay tháº¿:

| File/ThÆ° má»¥c | LÃ½ do khÃ´ng dÃ¹ng |
|--------------|------------------|
| `hive-site-template.xml` | Template cÅ©, Ä‘Ã£ Ä‘Æ°á»£c thay tháº¿ bá»Ÿi `config/hive-site.xml` |
| `hue-template.xml` | Template cÅ©, Ä‘Ã£ Ä‘Æ°á»£c thay tháº¿ bá»Ÿi `config/hue.ini` |
| `logs.txt` | File log cÅ©, cÃ³ thá»ƒ xÃ³a |
| `new_readme.md` | Readme draft cÅ© |
| `README_UI.md` | Readme UI cÅ© |
| `HUONG_DAN_SU_DUNG.md` | Duplicate vá»›i `final_readme.md` |
| `remove_data.py` | Script xÃ³a dá»¯ liá»‡u, Ã­t khi dÃ¹ng |
| `-p/` | ThÆ° má»¥c rá»—ng, cÃ³ thá»ƒ xÃ³a |
| `hue-data/` | Dá»¯ liá»‡u Hue tá»± sinh, cÃ³ thá»ƒ xÃ³a khi cáº§n reset |
| `hue-logs/` | Logs Hue tá»± sinh, cÃ³ thá»ƒ xÃ³a |
| `spark-apps/retail_etl_pipeline.py` | ETL cÅ© dÃ¹ng Hive, Ä‘Ã£ thay báº±ng `retail_etl_simple.py` |
| `spark-apps/customer_clustering.py` | Clustering cÅ©, Ä‘Ã£ thay báº±ng `customer_clustering_simple.py` |
| `spark-apps/product_recommendation.py` | Chá»©c nÄƒng recommendation chÆ°a hoÃ n thiá»‡n |
| `notebooks/` | Jupyter notebooks cho thá»­ nghiá»‡m, khÃ´ng báº¯t buá»™c |
| `hive-queries/` | SQL queries cho Hive, khÃ´ng cáº§n thiáº¿t vá»›i ETL simple |

---

## ğŸ“ Cáº¤U TRÃšC Dá»° ÃN CHÃNH

```
BigDataFinal/
â”œâ”€â”€ docker-compose.yml          # [QUAN TRá»ŒNG] Cáº¥u hÃ¬nh Docker services
â”œâ”€â”€ start.bat / start.sh        # [QUAN TRá»ŒNG] Khá»Ÿi Ä‘á»™ng há»‡ thá»‘ng
â”œâ”€â”€ stop.bat / stop.sh          # [QUAN TRá»ŒNG] Dá»«ng há»‡ thá»‘ng
â”œâ”€â”€ upload-data.bat             # [QUAN TRá»ŒNG] Upload data lÃªn HDFS
â”œâ”€â”€ run-etl.bat                 # [QUAN TRá»ŒNG] Cháº¡y ETL pipeline
â”œâ”€â”€ run-clustering.bat          # [QUAN TRá»ŒNG] Cháº¡y clustering
â”œâ”€â”€ run-webapp.bat              # [QUAN TRá»ŒNG] Khá»Ÿi Ä‘á»™ng webapp
â”œâ”€â”€ final_readme.md             # HÆ°á»›ng dáº«n sá»­ dá»¥ng
â”‚
â”œâ”€â”€ config/                     # Cáº¥u hÃ¬nh cÃ¡c services
â”‚   â”œâ”€â”€ hadoop.env              # Biáº¿n mÃ´i trÆ°á»ng Hadoop
â”‚   â”œâ”€â”€ hive-site.xml           # Cáº¥u hÃ¬nh Hive
â”‚   â””â”€â”€ hue.ini                 # Cáº¥u hÃ¬nh Hue
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ online_retail.csv       # [QUAN TRá»ŒNG] Dá»¯ liá»‡u gá»‘c
â”‚
â”œâ”€â”€ mongo-init/
â”‚   â””â”€â”€ init-mongo.js           # Script khá»Ÿi táº¡o MongoDB
â”‚
â”œâ”€â”€ spark-apps/                 # [QUAN TRá»ŒNG] Spark applications
â”‚   â”œâ”€â”€ retail_etl_simple.py    # ETL chÃ­nh (Ä‘ang dÃ¹ng)
â”‚   â””â”€â”€ customer_clustering_simple.py  # Clustering chÃ­nh (Ä‘ang dÃ¹ng)
â”‚
â””â”€â”€ webapp/                     # [QUAN TRá»ŒNG] Flask web application
    â”œâ”€â”€ app.py                  # Main Flask app
    â”œâ”€â”€ Dockerfile              # Docker build cho webapp
    â”œâ”€â”€ requirements.txt        # Python dependencies
    â””â”€â”€ templates/              # HTML templates
        â”œâ”€â”€ base.html           # Base template
        â”œâ”€â”€ index.html          # Dashboard chÃ­nh
        â”œâ”€â”€ customers.html      # Danh sÃ¡ch khÃ¡ch hÃ ng
        â”œâ”€â”€ customer_detail.html # Chi tiáº¿t khÃ¡ch hÃ ng
        â”œâ”€â”€ products.html       # Top sáº£n pháº©m
        â”œâ”€â”€ revenue.html        # PhÃ¢n tÃ­ch doanh thu
        â”œâ”€â”€ segments.html       # PhÃ¢n khÃºc khÃ¡ch hÃ ng
        â”œâ”€â”€ countries.html      # PhÃ¢n tÃ­ch theo quá»‘c gia
        â””â”€â”€ recommendations.html # Gá»£i Ã½ sáº£n pháº©m
```

---

## ğŸ”§ HÆ¯á»šNG DáºªN PHÃT TRIá»‚N CHI TIáº¾T

### 1. Docker Compose (`docker-compose.yml`)

Äá»‹nh nghÄ©a 12 services:

| Service | Port | MÃ´ táº£ |
|---------|------|-------|
| `namenode` | 9870, 9000 | HDFS NameNode |
| `datanode` | 9864 | HDFS DataNode |
| `postgres-metastore` | 5432 | PostgreSQL cho Hive Metastore |
| `hive-metastore` | 9083 | Hive Metastore service |
| `hive-server` | 10000 | HiveServer2 |
| `spark-master` | 8580, 7077 | Spark Master |
| `spark-worker` | 8581 | Spark Worker |
| `mongodb` | 27017 | MongoDB database |
| `mongo-express` | 8290 | MongoDB web UI |
| `hue` | 8788 | Hue web interface |
| `jupyter` | 8889 | Jupyter Notebook |
| `webapp` | 5000 | Flask web application |

**CÃ¡ch sá»­a Ä‘á»•i:**
```yaml
# Thay Ä‘á»•i port
ports:
  - "NEW_PORT:INTERNAL_PORT"

# Thay Ä‘á»•i memory
environment:
  - SPARK_WORKER_MEMORY=4g
```

---

### 2. ETL Pipeline (`spark-apps/retail_etl_simple.py`)

**Chá»©c nÄƒng chÃ­nh:**
1. Load dá»¯ liá»‡u tá»« CSV
2. LÃ m sáº¡ch dá»¯ liá»‡u (loáº¡i bá» null, Ä‘Æ¡n hÃ ng há»§y)
3. TÃ­nh toÃ¡n cÃ¡c metrics (RFM, revenue, top products)
4. LÆ°u vÃ o HDFS vÃ  MongoDB

**CÃ¡ch sá»­a Ä‘á»•i:**

```python
# Thay Ä‘á»•i input path
input_path = "/data/your_new_file.csv"

# ThÃªm collection má»›i
save_to_mongodb(your_df, "new_collection_name")

# ThÃªm phÃ¢n tÃ­ch má»›i
def analyze_new_metric(df):
    result = df.groupBy("column").agg(...)
    return result
```

**Collections MongoDB Ä‘Æ°á»£c táº¡o:**
- `customer_rfm` - RFM analysis
- `customer_segments` - Segment statistics
- `monthly_revenue`, `daily_revenue`, `hourly_revenue`
- `top_products_quantity`, `top_products_revenue`
- `country_performance`
- `monthly_trend`
- `transactions` (sample 10,000 records)

---

### 3. Clustering (`spark-apps/customer_clustering_simple.py`)

**Chá»©c nÄƒng:**
1. Äá»c dá»¯ liá»‡u tá»« HDFS
2. TÃ­nh RFM cho tá»«ng khÃ¡ch hÃ ng
3. GÃ¡n Ä‘iá»ƒm RFM (1-5 scale)
4. PhÃ¢n loáº¡i thÃ nh 8 phÃ¢n khÃºc

**8 PhÃ¢n khÃºc khÃ¡ch hÃ ng:**
- `Champions` - R,F,M cao
- `Loyal Customers` - F cao
- `New Customers` - R cao, F tháº¥p
- `At Risk` - R tháº¥p, F cao
- `Big Spenders` - M cao
- `Regular` - Trung bÃ¬nh
- `Hibernating` - R tháº¥p, F trung bÃ¬nh
- `Lost Customers` - R ráº¥t tháº¥p

**CÃ¡ch thÃªm phÃ¢n khÃºc má»›i:**
```python
def assign_segment(row):
    if condition:
        return "New Segment Name"
    # ...
```

---

### 4. Web Application (`webapp/app.py`)

**Framework:** Flask  
**Template Engine:** Jinja2  
**Database:** MongoDB (pymongo)

**Routes chÃ­nh:**

| Route | Template | MÃ´ táº£ |
|-------|----------|-------|
| `/` | `index.html` | Dashboard tá»•ng quan |
| `/customers` | `customers.html` | Danh sÃ¡ch khÃ¡ch hÃ ng |
| `/customer/<id>` | `customer_detail.html` | Chi tiáº¿t khÃ¡ch hÃ ng |
| `/products` | `products.html` | Top sáº£n pháº©m |
| `/revenue` | `revenue.html` | PhÃ¢n tÃ­ch doanh thu |
| `/segments` | `segments.html` | PhÃ¢n khÃºc khÃ¡ch hÃ ng |
| `/countries` | `countries.html` | PhÃ¢n tÃ­ch theo quá»‘c gia |
| `/recommendations` | `recommendations.html` | Gá»£i Ã½ sáº£n pháº©m |

**ThÃªm route má»›i:**
```python
@app.route('/new-page')
def new_page():
    data = list(db.your_collection.find())
    return render_template('new_page.html', data=data)
```

**ThÃªm template má»›i:**
1. Táº¡o file `webapp/templates/new_page.html`
2. Extend base template: `{% extends "base.html" %}`
3. ThÃªm route trong `app.py`

---

### 5. Káº¿t ná»‘i MongoDB

**Connection String:**
```python
MONGO_URI = "mongodb://admin:admin123@mongodb:27017/?authSource=admin"
client = MongoClient(MONGO_URI)
db = client.retail_analytics
```

**Truy váº¥n dá»¯ liá»‡u:**
```python
# Láº¥y táº¥t cáº£ documents
data = list(db.collection_name.find())

# Láº¥y vá»›i Ä‘iá»u kiá»‡n
data = list(db.collection_name.find({"field": "value"}))

# Aggregate
pipeline = [
    {"$group": {"_id": "$field", "count": {"$sum": 1}}}
]
result = list(db.collection_name.aggregate(pipeline))
```

---

## ğŸ”„ QUY TRÃŒNH PHÃT TRIá»‚N

### ThÃªm chá»©c nÄƒng phÃ¢n tÃ­ch má»›i:

1. **Sá»­a ETL** (`retail_etl_simple.py`):
   - ThÃªm function phÃ¢n tÃ­ch
   - Gá»i `save_to_mongodb(df, "collection_name")`

2. **ThÃªm Route** (`webapp/app.py`):
   - Táº¡o route má»›i
   - Query tá»« MongoDB collection

3. **Táº¡o Template** (`webapp/templates/`):
   - Táº¡o HTML file má»›i
   - Hiá»ƒn thá»‹ dá»¯ liá»‡u

4. **Cáº­p nháº­t Navigation** (`base.html`):
   - ThÃªm link vÃ o navbar

5. **Deploy**:
   ```bash
   docker-compose up -d --build webapp
   ```

---

## ğŸ› ï¸ DEBUG & TROUBLESHOOTING

### Xem logs:
```bash
docker logs spark-master --tail 100
docker logs webapp --tail 100
docker logs mongodb --tail 100
```

### Restart service:
```bash
docker restart webapp
docker restart spark-master
```

### Reset toÃ n bá»™:
```bash
docker-compose down -v
docker-compose up -d
```

### Kiá»ƒm tra MongoDB:
```bash
docker exec mongodb mongosh -u admin -p admin123 --authenticationDatabase admin
> use retail_analytics
> db.getCollectionNames()
> db.customer_rfm.findOne()
```

### Kiá»ƒm tra HDFS:
```bash
docker exec namenode hdfs dfs -ls /user/retail/
docker exec namenode hdfs dfs -cat /user/retail/file.txt
```

---

## ğŸ“š CÃ”NG NGHá»† Sá»¬ Dá»¤NG

| CÃ´ng nghá»‡ | Version | Má»¥c Ä‘Ã­ch |
|-----------|---------|----------|
| Apache Spark | 3.3.0 | Xá»­ lÃ½ dá»¯ liá»‡u lá»›n |
| Apache Hadoop | 3.3.5 | LÆ°u trá»¯ phÃ¢n tÃ¡n (HDFS) |
| Apache Hive | 3.1.3 | Data warehouse |
| MongoDB | 6.0 | NoSQL database |
| Flask | 2.3.3 | Web framework |
| Docker | 20.10+ | Containerization |
| Python | 3.9 | Programming language |
